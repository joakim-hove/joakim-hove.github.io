%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{0}



\title{ERT Documentation}
\date{Oct 28, 2018}
\release{2.3}
\author{Joakim Hove}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{manual::doc}}



\chapter{Introduction to ERT and Ensemble based methods}
\label{\detokenize{introduction/index:introduction-to-ert-and-ensemble-based-methods}}\label{\detokenize{introduction/index::doc}}\label{\detokenize{introduction/index:welcome-to-ert-s-documentation}}
The reservoir model for a green field is based on a range subsurface input
including seismic data, a geological concept, well logs and fluid samples. All
of this data is uncertain, and it is quite obvious that the resulting reservoir
model is quite uncertain. Although uncertain - reservoir models are still the
only tool we have when we make reservoir management decisions for the future.

Since reservoir models are very important for future predictions there is much
focus on reducing the uncertainty in the models. When the field has been in
production for some time one can use true data assembled from the producing
field to update the model. This process is commonly called \sphinxstyleemphasis{history matching} in
the petroleum industry, in this manual we will use the term \sphinxstyleemphasis{model updating}.
Before the model updating process can start you will need:
\begin{enumerate}
\item {} 
A reservoir model which has been \sphinxstyleemphasis{parameterized} with a parameter set
\(\{\lambda\}\).

\item {} 
Observation data from the producing field \(d\).

\end{enumerate}

Then the the actual model updating goes like this:
\begin{enumerate}
\item {} 
Simulate the behaviour of the field and assemble simulated data \(s\).

\item {} 
Compare the simulated simulated data \(s\) with the observed data \(d\).

\item {} 
Based on the misfit between \(s\) and \(d\) updated parameters are
\(\{\lambda'\}\) are calculated.

\end{enumerate}

Model updating falls into the general category of \sphinxstyleemphasis{inverse problems} - i.e. we
know the results and want to determine the input parameters which reproduce
these results. In statistical literature the phe process is often called
\sphinxstyleemphasis{conditioning}.

It is very important to remember that the sole reason for doing model updating
is to be able to make better predictions for the future, the history has
happened already anyway!


\section{Embrace the uncertainty}
\label{\detokenize{introduction/index:embrace-the-uncertainty}}
The main purpose of the model updating process is to reduce the uncertainty in
the description of the reservoir, however it is important to remember that the
goal is \sphinxstyleemphasis{not} to get rid of all the uncertainty and find ont true answer. There
are two reasons for this:
\begin{enumerate}
\item {} 
The data used when conditioning the model is also uncertain. E.g.
measurements of e.g. water cut and GOR is limited by the precision in the
measurement apparatus and also the allocation procedures. For example for 4D
seismic the uncertainty is large.

\item {} 
The model updating process will take place in the abstract space spanned by
the parameters \(\{\lambda\}\) - unless you are working on a synthetic
example the \sphinxstyleemphasis{real reservoir} is certainly not in this space.

\end{enumerate}

So the goal is to update the parameters \(\{\lambda\}\) so that the
simulations agree with the observations on average, with a variability which is
of the same order of magnitude as the uncertainty in the observations. The
assumption is then that if this model is used for predictions it will be
unbiased and give a realistic estimate of the future uncertainty. This
illustrated in figure \sphinxcode{\sphinxupquote{ensemble}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[scale=0.2]{{bpr}.jpg}
\caption{Ensemble plots before and after model updating, for one succesfull updating
and one updating which has gone wrong.}\label{\detokenize{introduction/index:id3}}\end{figure}

All the plots show simulations pressure in a cell as a function of time, with
measurements. Plots (1) and (3) show simulations before the model updating (i.e.
the \sphinxstyleemphasis{prior}) and plots (2) and (4) show the plots after the update process (the
\sphinxstyleemphasis{posterior}). The dashed vertical line is meant to illustrate the change from
history to prediction.

The left case with plots (1) and (2) is a succesfull history matching project.
The simulations from the posterior distribution are centered around the observed
values and the spread - i.e. uncertainty - is of the same order of magnitude as
the observation uncertainty. From this case we can reasonably expect that
predictions will be unbiased with an reasonable estimate of the uncertainty.

For the right hand case shown in plots (3) and (4) the model updating has \sphinxstyleemphasis{not}
been successfull and more work is required. Looking at the posterior solution we
see that the simulations are not centered around the observed values, when the
observed values from the historical period are not correctly reproduced there
is no reason to assume that the predictions will be correct either. Furthermore
we see that the uncertainty in the posterior case (4) is much smaller than the
uncertainty in the observations - this does not make sense; although our goal is
to reduce the uncertainty it should not be reduced significantly beyond the
uncertainty in the observations. The predictions from (4) will most probably be
biased and greatly underestimate the future uncertainty %
\begin{footnote}[1]\sphinxAtStartFootnote
: It should be emphasized that plots (3) and (4) show one simulated
quantity from an assumed larger set of observations, in general there
has been a different set of observations which has induced these large
and unwanted updates.
%
\end{footnote}.


\section{Ensemble Kalman Filter - EnKF}
\label{\detokenize{introduction/index:ensemble-kalman-filter-enkf}}
The ERT application was originally devised created to do model updating of
reservoir models with the EnKF algorithm. The experience from real world models
was that EnKF was not very suitable for reservoir applications, and ERT has
since changed to the Ensemble Smoother (ES) which can be said to be a simplified
version of the EnKF. But the characteristics of the EnKF algorithm still
influence many of the design decisions in ERT, it therefor makes sense to give a
short introduction to the Kalman Filter and EnKF.


\subsection{The Kalman Filter}
\label{\detokenize{introduction/index:the-kalman-filter}}
The Kalman FIlter originates in electronics the 60’s. The Kalman filter is
\sphinxstyleemphasis{widely} used, especially in applications where positioning is the goal - e.g.
the GPS system. The typical ingredients where the Kalman filter can be
interesting to try include:
\begin{enumerate}
\item {} 
We want to determine the final \sphinxstyleemphasis{state} of the system - this can typically the
the position.

\item {} 
The starting position is uncertain.

\item {} 
There is an \sphinxstyleemphasis{equation of motion} - or \sphinxstyleemphasis{forward model} - which describes how
the system evolves in time.

\item {} 
At fixed point in time we can \sphinxstyleemphasis{observe} the system, these observations are
uncertain.

\end{enumerate}

As a very simple application of the Kalman Filter, assume that we wish to
estimate the position of a boat as \(x(t)\); we know where the boat starts
(initial condition), we have an equation for how the boat moves with time and at
selected points in time \(t_k\) we get \sphinxstyleemphasis{measurements} of the position. The
quantities of interest are:
\begin{description}
\item[{\(x_k\)}] \leavevmode
The estimated position at time \(t_k\).

\item[{\(\sigma_k\)}] \leavevmode
The uncertainty in the position at time \(t_k\).

\item[{\(x_k^{\ast}\)}] \leavevmode
The \sphinxstyleemphasis{estimated/forecasted} position at time \(t_k\) -
this is the position estimated from \(x_{k-1}\) and \(f(x,t)\), but
before the observed data \(d_k\) is taken into account.

\item[{\(f(x,t)\)}] \leavevmode\begin{description}
\item[{The equation of motion - \sphinxstyleemphasis{forward model} - which propagates}] \leavevmode
\(x_{k-1} \to x_k^{\ast}\)

\end{description}

\end{description}

The purpose of the Kalman Filter is to determine an updated \(x_k\) from
\(x_{k-1}\) and \(d_k\). The updated \(x_k\) is the value which
\sphinxstyleemphasis{minimizes the variance} \(\sigma_k\). The equations for updated position
and uncertainty are:
\begin{equation*}
\begin{split}x_k = x_k^{\ast}\frac{\sigma_d^2}{\sigma_k^2 + \sigma_d^2} + x_d
\frac{\sigma_k^2}{\sigma_k^2 + \sigma_d^2}\end{split}
\end{equation*}\begin{equation*}
\begin{split}\sigma_k^2 = \sigma_k^i{2\ast}\left(1 - \frac{\sigma_k^{2\ast}}{\sigma_d^2 + \sigma_k^{2\ast}}\right)\end{split}
\end{equation*}
Looking at the equation for the position update we see that the analyyzed
position \(x_k\) is a weighted sum over the forecasted positon
\(x_k^{\ast}\) and measured position \(d_k\) - where the weighting
depends on the relative weight of the uncertainties \(sigma_k^{\ast}\) and
\(\sigma_d\). For the updated uncertainty the key take away message is that
the updated uncertainty will always be smaller than the forecasted uncertainty:
\(\sigma_k < \sigma_k^{\ast}\).


\subsection{Using an ensemble to estimate the uncertainty: EnKF}
\label{\detokenize{introduction/index:using-an-ensemble-to-estimate-the-uncertainty-enkf}}

\section{Ensemble Smoother - ES}
\label{\detokenize{introduction/index:ensemble-smoother-es}}

\chapter{The data types available in ERT}
\label{\detokenize{data_types/index:the-data-types-available-in-ert}}\label{\detokenize{data_types/index::doc}}
Very briefly described the purpose of ERT is to pass uncertain paramater values
to a simulator %
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxstyleemphasis{Simulator} should in this context be understood as the complete
forward model, including various pre and post processing steps in
addition to the actual reservoir simulation.
%
\end{footnote}, in a form which works as suitable input to the simulator and
then subsequently load the results from the simulator. This means that data must
be formatted in a form which the simulator can load, and also that ERT must be
able to read files generated by the simulator.

The data managed by ERT are organized in different \sphinxstyleemphasis{data types} described in
this chapter. Configuring the data used in the conditioning project is a very
important part of setting up a ERT configuration file - in practical terms this
is how you configure which uncertainty parameters should be studied. The data
types in ert can be categorized in two ways:
\begin{enumerate}
\item {} 
How the data type behaves dynamically: is it a static parameter like porosity
or a relperm parameter - i.e. does it serve as \sphinxstyleemphasis{input} to the simulator, or
is it a quantity which is generated as a result from the simulation. When
understanding the model updating algorithm and process it is important to
understand this difference properly.

\item {} 
How the data type is implemented, what type of files does it and read write,
how is it configured and so on.

\end{enumerate}


\section{Parameters}
\label{\detokenize{data_types/index:parameters}}

\subsection{Scalar parameters with a template: GEN\_KW}
\label{\detokenize{data_types/index:scalar-parameters-with-a-template-gen-kw}}

\subsection{3D field parameters: FIELD}
\label{\detokenize{data_types/index:d-field-parameters-field}}

\subsection{2D Surface parameters: SURFACE}
\label{\detokenize{data_types/index:d-surface-parameters-surface}}

\subsection{General vector parameters: GEN\_PARAM}
\label{\detokenize{data_types/index:general-vector-parameters-gen-param}}

\section{Simulated data}
\label{\detokenize{data_types/index:simulated-data}}

\subsection{Summary: SUMMARY}
\label{\detokenize{data_types/index:summary-summary}}

\subsection{General data: GEN\_DATA}
\label{\detokenize{data_types/index:general-data-gen-data}}

\subsection{Keyword results: CUSTOM\_KW}
\label{\detokenize{data_types/index:keyword-results-custom-kw}}

\section{EnKF heritage}
\label{\detokenize{data_types/index:enkf-heritage}}
With regards to the datatypes in ERT this is a part of the application where the
EnKF heritage shows through quite clearly, the datetypes offered by ERT would
probably be different if ERT was made for Ensemble Smoother from the outset.
Pecularites of EnKF heritage include:
\begin{enumerate}
\item {} 
The \sphinxtitleref{FIELD} implementation can behave both as a dynamic quantity, i.e.
pressure and saturation, and static property like porosity. In ERT it is
currently \sphinxstyleemphasis{only used} as a parameter.

\item {} 
The parameter types have an internal pseudo time dependence corresponding to
the “update time” induced by the EnKF scheme. This pseudo time dependence is
not directly exposed to the user, but it is still part of the implementation
and e.g. when writing plugins which work with parameter data managed by ERT
you must relate to it.

\item {} 
The time dependence of the \sphinxtitleref{GEN\_DATA} implementation. This is just too
complex, there have been numerous problems with people who configure the
\sphinxtitleref{GEN\_DATA} keywords incorrectly.

\end{enumerate}


\section{Observations}
\label{\detokenize{data_types/index:observations}}

\chapter{Running simulations - the Forward Model}
\label{\detokenize{forward_model/index::doc}}\label{\detokenize{forward_model/index:running-simulations-the-forward-model}}

\section{The forward model}
\label{\detokenize{forward_model/index:the-forward-model}}

\subsection{Default jobs}
\label{\detokenize{forward_model/index:default-jobs}}

\subsubsection{Reservoir simulation: eclipse}
\label{\detokenize{forward_model/index:reservoir-simulation-eclipse}}

\subsubsection{Reservoir modelling: RMS}
\label{\detokenize{forward_model/index:reservoir-modelling-rms}}

\subsubsection{File system utilities}
\label{\detokenize{forward_model/index:file-system-utilities}}

\subsection{Configuring your own jobs}
\label{\detokenize{forward_model/index:configuring-your-own-jobs}}

\subsection{The \sphinxtitleref{job\_dispatch} executable}
\label{\detokenize{forward_model/index:the-job-dispatch-executable}}

\section{Interfacing with the cluster}
\label{\detokenize{forward_model/index:interfacing-with-the-cluster}}

\chapter{Workflows}
\label{\detokenize{workflows/index::doc}}\label{\detokenize{workflows/index:workflows}}

\section{External workflows}
\label{\detokenize{workflows/index:external-workflows}}

\section{Internal workflows}
\label{\detokenize{workflows/index:internal-workflows}}

\section{Plugins}
\label{\detokenize{workflows/index:plugins}}

\chapter{Configuring observations for ERT}
\label{\detokenize{observations/index:configuring-observations-for-ert}}\label{\detokenize{observations/index::doc}}

\section{General overview}
\label{\detokenize{observations/index:general-overview}}
When using ERT to condition on dynamic data, it is necessary to
specify which data to condition on. In particular, for a given piece
of data to condition on, the ERT  application needs to know:
\begin{itemize}
\item {} 
The actual measured value of the data.

\item {} 
The uncertainty of the measured data.

\item {} 
The time of measurement.

\item {} 
How to simulate a response of the data given a parametrized ECLIPSE model.

\end{itemize}

To provide this observation to ERT, an observation file must be
created. The observation file is a plain text file, and is in essence
built around for different classes of observations and has an
associated keyword for each class:
\begin{itemize}
\item {} 
Well or group rates from an existing ECLIPSE reference case: The
HISTORY\_OBSERVATION keyword.

\item {} 
Well logs, RFTS and PLTs: The BLOCK\_OBSERVATION keyword.

\item {} 
Separator tests, region pressures, etc.: The SUMMARY\_OBSERVATION
keyword.

\item {} 
Exotic observations (e.g. data from 4D seismic): The
GENERAL\_OBSERVATION keyword.

\end{itemize}


\section{The HISTORY\_OBSERVATION keyword}
\label{\detokenize{observations/index:the-history-observation-keyword}}
The keyword HISTORY\_OBSERVATION is used to condition on observations
from the WCONHIST and WCONINJH keywords in schedule file provided to
the enkf project (or alternatively an ECLIPSE summary file if you have
changed the HISTORY\_SOURCE keyword in the enkf project). The keyword
is typically used to condition on production and injection rates for
groups and wells, as well as bottom hole and tubing head pressures. An
observation entered with the HISTORY\_OBSERVATION keyword will be
active at all report steps where data for the observation can be
found.

In it’s simplest form, a history observation is created as follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WOPR}\PYG{p}{:}\PYG{n}{P1}\PYG{p}{;}
\end{sphinxVerbatim}

This will condition on WOPR in well P1 using a default observation
error. The default observation error is a relative error of 10\% to the
measurement with a minimum error of 0.10. See below on how explicitly
set the error.

In general, to condition on variable VAR in well or group WGNAME, one
uses:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{VAR}\PYG{p}{:}\PYG{n}{WGNAME}\PYG{p}{;}
\end{sphinxVerbatim}

Note that there must be a colon “:” between VAR and WGNAME and that
the statement shall end with a semi-colon “;”. Thus, to condition on
WOPR, WWCT and WGOR in well C-17, and for the GOPR for the whole
field, one would add the following to the observation configuration:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WOPR}\PYG{p}{:}\PYG{n}{C}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{17}\PYG{p}{;}
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WWCT}\PYG{p}{:}\PYG{n}{C}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{17}\PYG{p}{;}
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WGOR}\PYG{p}{:}\PYG{n}{C}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{17}\PYG{p}{;}

\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{GOPR}\PYG{p}{:}\PYG{n}{FIELD}\PYG{p}{;}
\end{sphinxVerbatim}

By default, the observation error is set to 10\% of the observed value,
with a minimum of 0.10. It can be changed as follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{GOPR}\PYG{p}{:}\PYG{n}{FIELD}
\PYG{p}{\PYGZob{}}
   \PYG{n}{ERROR}       \PYG{o}{=} \PYG{l+m+mi}{1000}\PYG{p}{;}
   \PYG{n}{ERROR\PYGZus{}MODE}  \PYG{o}{=} \PYG{n}{ABS}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

This will set the observation error to 1000 for all observations of
GOPR:FIELD. Note that both the items ERROR and ERROR\_MODE as well as
the whole definition shall end with a semi-colon.

The item ERROR\_MODE can take three different values: ABS, REL or
RELMIN. If set to REL, all observation errors will be set to the
observed values multiplied by ERROR. Thus, the following will
condition on water injection rate for the whole field with 20\%
observation uncertainity:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{GWIR}\PYG{p}{:}\PYG{n}{FIELD}
\PYG{p}{\PYGZob{}}
   \PYG{n}{ERROR}       \PYG{o}{=} \PYG{l+m+mf}{0.20}\PYG{p}{;}
   \PYG{n}{ERROR\PYGZus{}MODE}  \PYG{o}{=} \PYG{n}{REL}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

If you do not want the observation error to drop below a given
threshold, say 100, you can use RELMIN and the keyword ERROR\_MIN:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{GWIR}\PYG{p}{:}\PYG{n}{FIELD}
\PYG{p}{\PYGZob{}}
   \PYG{n}{ERROR}       \PYG{o}{=} \PYG{l+m+mf}{0.20}\PYG{p}{;}
   \PYG{n}{ERROR\PYGZus{}MODE}  \PYG{o}{=} \PYG{n}{RELMIN}\PYG{p}{;}
   \PYG{n}{ERROR\PYGZus{}MIN}   \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

Note that the configuration parser does not threat carriage return
different from space. Thus, the following statement is equivalent to
the previous:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{GWIR}\PYG{p}{:}\PYG{n}{FIELD} \PYG{p}{\PYGZob{}} \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mf}{0.20}\PYG{p}{;} \PYG{n}{ERROR\PYGZus{}MODE} \PYG{o}{=} \PYG{n}{RELMIN}\PYG{p}{;} \PYG{n}{ERROR\PYGZus{}MIN} \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

Also note that the special keyword include can be used to read an
external file. This can be very useful if you want to change the
standard configuration for a lot of observations in one go. For
example, consider the following code:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WOPR}\PYG{p}{:}\PYG{n}{P1} \PYG{p}{\PYGZob{}} \PYG{n}{include} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hist\PYGZus{}obs\PYGZus{}wells.txt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WOPR}\PYG{p}{:}\PYG{n}{P2} \PYG{p}{\PYGZob{}} \PYG{n}{include} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hist\PYGZus{}obs\PYGZus{}wells.txt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WOPR}\PYG{p}{:}\PYG{n}{P3} \PYG{p}{\PYGZob{}} \PYG{n}{include} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hist\PYGZus{}obs\PYGZus{}wells.txt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WOPR}\PYG{p}{:}\PYG{n}{P4} \PYG{p}{\PYGZob{}} \PYG{n}{include} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hist\PYGZus{}obs\PYGZus{}wells.txt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{WOPR}\PYG{p}{:}\PYG{n}{P5} \PYG{p}{\PYGZob{}} \PYG{n}{include} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hist\PYGZus{}obs\PYGZus{}wells.txt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

Where the contents of the file hist\_obs\_wells.txt may be something
like:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ERROR\PYGZus{}MODE}  \PYG{o}{=} \PYG{n}{RELMIN}\PYG{p}{;}
\PYG{n}{ERROR}       \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{;}
\PYG{n}{ERROR\PYGZus{}MIN}   \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}
\end{sphinxVerbatim}

In this case, changing the file hist\_obs\_wells.txt will affect all of
the observations.

Note that the keyword include can be used anywhere in the
configuration file. However, nested inclusion (use of include in a
file that has already been included with include) is not allowed.

By default, an observation entered with the HISTORY\_OBSERVATION
keyword will get the observed values, i.e. the ‘true’ values, from the
WCONHIST and WCONINJH keywords in the schedule file provided to the
ERT project. However it also possible to get the observed values from
a reference case. In that case you must set set HISTORY\_SOURCE
variable in the ERT configuration file, see Creating a configuration
file for ERT.

To change the observation error for a HISTORY\_OBSERVATION for one or
more segments of the historic period, you can use the SEGMENT
keyword. For example:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HISTORY\PYGZus{}OBSERVATION} \PYG{n}{GWIR}\PYG{p}{:}\PYG{n}{FIELD}
\PYG{p}{\PYGZob{}}
   \PYG{n}{ERROR}       \PYG{o}{=} \PYG{l+m+mf}{0.20}\PYG{p}{;}
   \PYG{n}{ERROR\PYGZus{}MODE}  \PYG{o}{=} \PYG{n}{RELMIN}\PYG{p}{;}
   \PYG{n}{ERROR\PYGZus{}MIN}   \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}

   \PYG{n}{SEGMENT} \PYG{n}{FIRST\PYGZus{}YEAR}
   \PYG{p}{\PYGZob{}}
      \PYG{n}{START} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
      \PYG{n}{STOP}  \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{;}
      \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mf}{0.50}\PYG{p}{;}
      \PYG{n}{ERROR\PYGZus{}MODE} \PYG{o}{=} \PYG{n}{REL}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}\PYG{p}{;}

   \PYG{n}{SEGMENT} \PYG{n}{SECOND\PYGZus{}YEAR}
   \PYG{p}{\PYGZob{}}
      \PYG{n}{START}      \PYG{o}{=} \PYG{l+m+mi}{11}\PYG{p}{;}
      \PYG{n}{STOP}       \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{;}
      \PYG{n}{ERRROR}     \PYG{o}{=} \PYG{l+m+mi}{1000}\PYG{p}{;}
      \PYG{n}{ERROR\PYGZus{}MODE} \PYG{o}{=} \PYG{n}{ABS}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

The items START and STOP sets the start and stop of the segment in
terms of ECLIPSE restart steps. The keywords ERROR, ERROR\_MODE and
ERROR\_MIN behaves like before. If the segments overlap, they are
computed in alphabetical order.  Error covariance for “merged” updates

When merging the historical observations from several report steps
together in one update the different steps are not independent, and it
is beneficial to use a error covariance matrix, by using the keywords
AUTO\_CORRF and AUTO\_CORRF\_PARAM ERT will automatically estimate a
error-covariance matrix based on the auto correlation function
specified by the AUTO\_CORRF keyword, with the parameter given by the
AUTO\_CORRF\_PARAM parameter (i.e. the auto correlation length). The
currently available auto correlation functions are:
\begin{quote}

EXP   \textasciitilde{} exp(-x)
GAUSS \textasciitilde{} exp(-x*x/2)
\end{quote}

where the parameter x is given as:
\begin{quote}

x = (t2 - t1) / AUTO\_CORRF\_PARAM
\end{quote}


\section{The SUMMARY\_OBSERVATION keyword}
\label{\detokenize{observations/index:the-summary-observation-keyword}}
The keyword SUMMARY\_OBSERVATION can be used to condition on any
observation whos simulated value is written to the ECLIPSE summary
file, e.g. well rates, region properties, group and field rates etc. A
quite typical usage of SUMMARY\_OBSERVATION is to condition on the
results of a separator test.

Note: Although it is possible to condition on well and group rates
with SUMMARY\_OBSERVATION, it is usually easier to use
HISTORY\_OBSERVATION for this.

In order to create a summary observation, four pieces of information
are needed: The observed value, the observation error, the time of
observation and a summary key. A typical summary observation is
created as follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{SUMMARY\PYGZus{}OBSERVATION} \PYG{n}{SEP\PYGZus{}TEST\PYGZus{}2005}
\PYG{p}{\PYGZob{}}
   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}
   \PYG{n}{ERROR} \PYG{o}{=}   \PYG{l+m+mi}{5}\PYG{p}{;}
   \PYG{n}{DATE}  \PYG{o}{=} \PYG{l+m+mi}{21}\PYG{o}{/}\PYG{l+m+mi}{08}\PYG{o}{/}\PYG{l+m+mi}{2005}\PYG{p}{;}
   \PYG{n}{KEY}   \PYG{o}{=} \PYG{n}{GOPR}\PYG{p}{:}\PYG{n}{BRENT}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

This will create an observation of group oil production for the brent
group on 21th of august 2005. The observed value was 100 with a
standard deviation of 5. The name SEP\_TEST\_2005 will be used as a
label for the observation within the ERT and must be unique.

Similarly to the name of a HISTORY\_OBSERVATION, the item KEY in a
SUMMARY\_OBSERVATION is used to look up the simulated value from the
summary file. And again, as when declaring a HISTORY\_OBSERVATION, to
condition on VAR in well, group or region WGRNAME, one uses:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{KEY} \PYG{o}{=} \PYG{n}{VAR}\PYG{p}{:}\PYG{n}{WGRNAME}\PYG{p}{;}
\end{sphinxVerbatim}

For example, to condition on RPPW in region 8, one uses:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{KEY} \PYG{o}{=} \PYG{n}{RPPW}\PYG{p}{:}\PYG{l+m+mi}{8}\PYG{p}{;}
\end{sphinxVerbatim}

It is also possible to give the observation time as a restart number
using the RESTART item or as time in days from simulation start using
the DAYS item. Here are two examples:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Giving} \PYG{n}{the} \PYG{n}{observation} \PYG{n}{time} \PYG{o+ow}{in} \PYG{n}{terms} \PYG{n}{of} \PYG{n}{restart} \PYG{n}{number}\PYG{o}{.}
\PYG{n}{SUMMARY\PYGZus{}OBSERVATION} \PYG{n}{SEP\PYGZus{}TEST\PYGZus{}2005}
\PYG{p}{\PYGZob{}}
   \PYG{n}{VALUE}    \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}
   \PYG{n}{ERROR}    \PYG{o}{=}   \PYG{l+m+mi}{5}\PYG{p}{;}
   \PYG{n}{RESTART}  \PYG{o}{=}  \PYG{l+m+mi}{42}\PYG{p}{;}
   \PYG{n}{KEY}      \PYG{o}{=} \PYG{n}{GOPR}\PYG{p}{:}\PYG{n}{BRENT}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}


\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Giving} \PYG{n}{the} \PYG{n}{observation} \PYG{n}{time} \PYG{o+ow}{in} \PYG{n}{terms} \PYG{n}{of} \PYG{n}{days}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{k+kn}{from} \PYG{n+nn}{simulation} \PYG{n}{start}\PYG{o}{.}
\PYG{n}{SUMMARY\PYGZus{}OBSERVATION} \PYG{n}{SEP\PYGZus{}TEST\PYGZus{}2008}
\PYG{p}{\PYGZob{}}
   \PYG{n}{VALUE}    \PYG{o}{=} \PYG{l+m+mi}{213}\PYG{p}{;}
   \PYG{n}{ERROR}    \PYG{o}{=}  \PYG{l+m+mi}{10}\PYG{p}{;}
   \PYG{n}{DAYS}     \PYG{o}{=} \PYG{l+m+mi}{911}\PYG{p}{;}
   \PYG{n}{KEY}      \PYG{o}{=} \PYG{n}{GOPR}\PYG{p}{:}\PYG{n}{NESS}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}


\section{The BLOCK\_OBSERVATION keyword}
\label{\detokenize{observations/index:the-block-observation-keyword}}
This is observations of variables in grid blocks/cells. The
observations can be of arbitrary ECLIPSE fields like PRESSURE
(typically for an RFT), PORO or PERM. A block observation is entered
with the BLOCK\_OBSERVATION keyword. Here is an example of a typical
block observation:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{BLOCK\PYGZus{}OBSERVATION} \PYG{n}{RFT\PYGZus{}2006}
\PYG{p}{\PYGZob{}}
   \PYG{n}{FIELD} \PYG{o}{=} \PYG{n}{PRESSURE}\PYG{p}{;}
   \PYG{n}{DATE}  \PYG{o}{=} \PYG{l+m+mi}{22}\PYG{o}{/}\PYG{l+m+mi}{10}\PYG{o}{/}\PYG{l+m+mi}{2006}\PYG{p}{;}

   \PYG{n}{OBS} \PYG{n}{P1} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
   \PYG{n}{OBS} \PYG{n}{P2} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{101}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
   \PYG{n}{OBS} \PYG{n}{P3} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{102}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

This will condition on observations of the pressure in grid blocks
(1,1,1), (2,2,1) and (2,3,1) on the 22/10/2006.

By default the BLOCK\_OBSERVATION requires that the specific field
which has been observed (e.g. PRESSURE in the example above) must have
been specified in main ERT configuration file using the FIELD keyword,
and ECLIPSE must be configured to produce a restart file for this
particular time. Alternatively it is possible to tell ERT to use the
summary vector as source of the data:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{BLOCK\PYGZus{}OBSERVATION} \PYG{n}{RFT\PYGZus{}2006}
\PYG{p}{\PYGZob{}}
   \PYG{n}{FIELD} \PYG{o}{=} \PYG{n}{PRESSURE}\PYG{p}{;}
   \PYG{n}{DATE}  \PYG{o}{=} \PYG{l+m+mi}{22}\PYG{o}{/}\PYG{l+m+mi}{10}\PYG{o}{/}\PYG{l+m+mi}{2006}\PYG{p}{;}
   \PYG{n}{SOURCE} \PYG{o}{=} \PYG{n}{SUMMARY}\PYG{p}{;}

   \PYG{n}{OBS} \PYG{n}{P1} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
   \PYG{n}{OBS} \PYG{n}{P2} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{101}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
   \PYG{n}{OBS} \PYG{n}{P3} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{102}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

In this case the data will be loaded from the BPR vectors in the
summary file.

Note the use of the sub class OBS to specify the actUal observed
values, the observation errors and their grid location. Each OBS shall
have a unique key within the BLOCK\_OBSERVATION instance, and is
required to have the items I, J, K, VALUE and ERROR. These are the
grid i,j and k indicies for the observation point, the observed value
and it’s standard deviation.

As with a SUMMARY\_OBSERVATION, the observation time can be given as
either a date, days since simulation start or restart number. The
respective keys for setting giving it as date, days or restart number
are DATE, DAYS and RESTART. Note that each BLOCK\_OBSERVATION instance
must have an unique global name (RFT\_2006 in the example above).

Block observations can often be quite long. Thus, it is often a good
idea to use the special keyword include in order to store the OBS
structures in a different file. This is done as follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{BLOCK\PYGZus{}OBSERVATION} \PYG{n}{RFT\PYGZus{}2006}
\PYG{p}{\PYGZob{}}
   \PYG{n}{FIELD}   \PYG{o}{=} \PYG{n}{PRESSURE}\PYG{p}{;}
   \PYG{n}{RESTART} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{;}

   \PYG{n}{include} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RFT\PYGZus{}2006\PYGZus{}OBS\PYGZus{}DATA.txt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

Where the file RFT\_2006\_OBS\_DATA.txt contains the OBS instances:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{OBS} \PYG{n}{P1} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{100}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{OBS} \PYG{n}{P2} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{101}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{OBS} \PYG{n}{P3} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{112}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{OBS} \PYG{n}{P4} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{122}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{OBS} \PYG{n}{P5} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{4}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{112}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\PYG{n}{OBS} \PYG{n}{P6} \PYG{p}{\PYGZob{}} \PYG{n}{I} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;}  \PYG{n}{J} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{;}  \PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}   \PYG{n}{VALUE} \PYG{o}{=} \PYG{l+m+mi}{122}\PYG{p}{;}  \PYG{n}{ERROR} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{;} \PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}


\section{The GENERAL\_OBSERVATION keyword}
\label{\detokenize{observations/index:the-general-observation-keyword}}
The GENERAL\_OBSERVATION keyword is used together with the GEN\_DATA and
GEN\_PARAM type. This pair of observation and data types are typically
used when you want to update something special which does not fit into
any of the predefined enkf types. The ERT application just treats
GENERAL\_OBSERVATION (and also GEN\_DATA) as a range of number with no
particular structure, this is very flexible, but of course also a bit
more complex to use:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GENERAL\PYGZus{}OBSERVATION} \PYG{n}{GEN\PYGZus{}OBS1}\PYG{p}{\PYGZob{}}
   \PYG{n}{DATA}     \PYG{o}{=} \PYG{n}{SOME\PYGZus{}FIELD}\PYG{p}{;}
   \PYG{n}{RESTART}  \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{;}
   \PYG{n}{OBS\PYGZus{}FILE} \PYG{o}{=} \PYG{n}{some\PYGZus{}file}\PYG{o}{.}\PYG{n}{txt}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

This example a minimum GENERAL\_OBSERVATION. The keyword DATA points to
the GEN\_DATA instance this observation is ‘observing’, RESTART gives
the report step when this observation is active. OBS\_FILE should be
the name of a file with observation values, and the corresponding
uncertainties. The file with observations should just be a plain text
file with numbers in it, observations and corresponding uncertainties
interleaved. An example of an OBS\_FILE:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mf}{1.46} \PYG{l+m+mf}{0.26}
\PYG{l+m+mf}{25.0} \PYG{l+m+mf}{5.0}
\PYG{l+m+mf}{5.00} \PYG{l+m+mf}{1.00}
\end{sphinxVerbatim}

This OBS\_FILE has three observations: 1.46 +/- 0.26, 25.0 +/- 5.0 and
5.00 +/- 1.00. In the example above it is assumed that the DATA
instance we are observing (i.e. comparing with) has the same number of
elements as the observation, i.e. three in this case. By using the
keywords INDEX\_LIST or INDEX\_FILE you can select the elements of the
GEN\_DATA instance you are interested in. Consider for example:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GENERAL\PYGZus{}OBSERVATION} \PYG{n}{GEN\PYGZus{}OBS1}\PYG{p}{\PYGZob{}}
   \PYG{n}{DATA}       \PYG{o}{=} \PYG{n}{SOME\PYGZus{}FIELD}\PYG{p}{;}
   \PYG{n}{INDEX\PYGZus{}LIST} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{9}\PYG{p}{;}
   \PYG{n}{RESTART}    \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{;}
   \PYG{n}{OBS\PYGZus{}FILE}   \PYG{o}{=} \PYG{n}{some\PYGZus{}file}\PYG{o}{.}\PYG{n}{txt}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

Here we use INDEX\_LIST to indicate that we are interested in element
0,3 and 9 of the GEN\_DATA instance:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GEN\PYGZus{}DATA}                     \PYG{n}{GEN\PYGZus{}OBS1}
\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}                     \PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{=}
\PYG{l+m+mf}{1.56} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{1.46}  \PYG{l+m+mf}{0.26}
\PYG{l+m+mf}{23.0}        \PYG{o}{/}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{25.0}   \PYG{l+m+mf}{5.00}
\PYG{l+m+mf}{56.0}        \PYG{o}{\textbar{}}    \PYG{o}{/}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{5.00}  \PYG{l+m+mf}{1.00}
\PYG{l+m+mf}{27.0} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{/}    \PYG{o}{\textbar{}}           \PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{=}
 \PYG{l+m+mf}{0.2}             \PYG{o}{\textbar{}}
\PYG{l+m+mf}{1.56}             \PYG{o}{\textbar{}}
\PYG{l+m+mf}{1.78}             \PYG{o}{\textbar{}}
\PYG{l+m+mf}{6.78}             \PYG{o}{\textbar{}}
\PYG{l+m+mf}{9.00}             \PYG{o}{\textbar{}}
\PYG{l+m+mf}{4.50} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{/}
\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}
\end{sphinxVerbatim}

In addition to INDEX\_LIST it is possible to use INDEX\_FILE which
should just point at an plain text file with indexes (without any ‘,’
or anything). Finally, if your observation only has one value, you can
embed it in the config object with VALUE and ERROR.


\subsection{Matching GEN\_OBS and GEN\_DATA}
\label{\detokenize{observations/index:matching-gen-obs-and-gen-data}}
It is important to match up the GEN\_OBS observations with the
corresponding GEN\_DATA simulation data correctly. The GEN\_DATA result
files must have an embedded ‘\%d’ to indicate the report step in them -
in the case of smoother based workflows the actual numerical value
here is not important. To ensure that GEN\_OBS and corresponding
GEN\_DATA values match up correctly only the RESTART method is allowed
for GEN\_OBS when specifying the time. So consider a setup like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Config} \PYG{n}{file}\PYG{p}{:}
\PYG{n}{GEN\PYGZus{}DATA} \PYG{n}{RFT\PYGZus{}BH67} \PYG{n}{INPUT\PYGZus{}FORMAT}\PYG{p}{:}\PYG{n}{ASCII} \PYG{n}{RESULT\PYGZus{}FILE}\PYG{p}{:}\PYG{n}{rft\PYGZus{}BH67\PYGZus{}}\PYG{o}{\PYGZpc{}}\PYG{n}{d}    \PYG{n}{REPORT\PYGZus{}STEPS}\PYG{p}{:}\PYG{l+m+mi}{20}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}                                                       \PYG{o}{/}\PYG{o}{\textbar{}}\PYGZbs{}                \PYG{o}{/}\PYG{o}{\textbar{}}\PYGZbs{}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}                                                        \PYG{o}{\textbar{}}                  \PYG{o}{\textbar{}}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Observation} \PYG{n}{file}\PYG{p}{:}                                       \PYG{o}{\textbar{}}                  \PYG{o}{\textbar{}}
\PYG{n}{GENERAL\PYGZus{}OBSERVATION} \PYG{n}{GEN\PYGZus{}OBS1}\PYG{p}{\PYGZob{}}                              \PYG{o}{+}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{/}
   \PYG{n}{DATA}       \PYG{o}{=} \PYG{n}{RFT\PYGZus{}BH67}\PYG{p}{;}                                  \PYG{o}{\textbar{}}
   \PYG{n}{RESTART}    \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{;}   \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{o}{/}
   \PYG{n}{OBS\PYGZus{}FILE}   \PYG{o}{=} \PYG{n}{some\PYGZus{}file}\PYG{o}{.}\PYG{n}{txt}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}
\end{sphinxVerbatim}

Here we see that the observation is active at report step 20, and we
expect the forward model to create a file rft\_BH67\_20 in each
realization directory.  Error covariance

The optional keyword ERROR\_COVAR can be used to point to an existing
file, containing an error covariance matrix. The file should contain
the elements of the matrix as formatted numbers; newline formatting is
allowed but not necessary. Since the matrix should by construction be
symmetric there is no difference between column-major and row-major
order! The covariance matrix
\begin{quote}

{[} 1      0.75  -0.25{]}
\end{quote}
\begin{description}
\item[{C =  {[} 0.75   1.25  -0.50{]}}] \leavevmode
{[}-0.25  -0.50   0.85{]}

\end{description}

Can be represented by the file:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{1}
\PYG{l+m+mf}{0.75}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}
\PYG{l+m+mf}{0.75}
\PYG{l+m+mf}{1.25}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.50}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.50}
\PYG{l+m+mf}{0.85}
\end{sphinxVerbatim}

without newlines, or alternatively:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{1}       \PYG{l+m+mf}{0.75}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}
\PYG{l+m+mf}{0.75}    \PYG{l+m+mf}{1.25}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.50}
\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.50}   \PYG{l+m+mf}{0.85}
\end{sphinxVerbatim}

with newlines.


\chapter{The smoother update in ERT}
\label{\detokenize{update/index:the-smoother-update-in-ert}}\label{\detokenize{update/index::doc}}

\section{Analysing the results}
\label{\detokenize{update/index:analysing-the-results}}

\section{Local updates}
\label{\detokenize{update/index:local-updates}}

\section{Advanced: implementing your own update}
\label{\detokenize{update/index:advanced-implementing-your-own-update}}

\chapter{Use the ERT API to create custom functionality}
\label{\detokenize{scripting/index:use-the-ert-api-to-create-custom-functionality}}\label{\detokenize{scripting/index::doc}}

\chapter{Keywords for the configuration file}
\label{\detokenize{keywords/index::doc}}\label{\detokenize{keywords/index:ert-kw-full-doc}}\label{\detokenize{keywords/index:keywords-for-the-configuration-file}}
\DUrole{xref,std,std-ref}{Go to main ERT page}


\section{General overview}
\label{\detokenize{keywords/index:general-overview}}
The enkf application is started with a single argument, which is the name of the
configuration file to be used. The enkf configuration file serves several
purposes, which are:
\begin{itemize}
\item {} 
Defining which ECLIPSE model to use, i.e. giving a data, grid and schedule file.

\item {} 
Defining which observation file to use.

\item {} 
Defining how to run simulations.

\item {} 
Defining where to store results.

\item {} 
Creating a parametrization of the ECLIPSE model.

\end{itemize}

The configuration file is a plain text file, with one statement per line. The
first word on each line is a keyword, which then is followed by a set of
arguments that are unique to the particular keyword. Except for the DEFINE
keyword, ordering of the keywords is not significant. Similarly to ECLIPSE data
files, lines starting with “\textendash{}” are treated as comments.

The keywords in the enkf configuration file can roughly be divded into two
groups:
\begin{itemize}
\item {} 
Basic required keywords not related to parametrization. I.e., keywords giving
the data, grid, schedule and observation file, defining how to run simulations
and how to store results. These keywords are described in {\hyperref[\detokenize{keywords/index:id1}]{\sphinxcrossref{\DUrole{std,std-ref}{Basic required
keywords.}}}}

\item {} 
Basic optional keywords not related to parametrization. These keywords are
described in {\hyperref[\detokenize{keywords/index:id2}]{\sphinxcrossref{\DUrole{std,std-ref}{Basic optional keywords}}}}.

\item {} 
Keywords related to parametrization of the ECLIPSE model. These keywords are
described in {\hyperref[\detokenize{keywords/index:id4}]{\sphinxcrossref{\DUrole{std,std-ref}{Parametrization keywords}}}}.

\item {} 
Advanced keywords not related to parametrization. These keywords are described
in {\hyperref[\detokenize{keywords/index:id11}]{\sphinxcrossref{\DUrole{std,std-ref}{Advanced optional keywords}}}}.

\end{itemize}


\section{List of keywords}
\label{\detokenize{keywords/index:list-of-keywords}}

\begin{savenotes}\sphinxatlongtablestart\begin{longtable}{|l|l|l|l|}
\hline
\sphinxstyletheadfamily 
Keyword name
&\sphinxstyletheadfamily 
Required by user?
&\sphinxstyletheadfamily 
Default value
&\sphinxstyletheadfamily 
Purpose
\\
\hline
\endfirsthead

\multicolumn{4}{c}%
{\makebox[0pt]{\sphinxtablecontinued{\tablename\ \thetable{} -- continued from previous page}}}\\
\hline
\sphinxstyletheadfamily 
Keyword name
&\sphinxstyletheadfamily 
Required by user?
&\sphinxstyletheadfamily 
Default value
&\sphinxstyletheadfamily 
Purpose
\\
\hline
\endhead

\hline
\multicolumn{4}{r}{\makebox[0pt][r]{\sphinxtablecontinued{Continued on next page}}}\\
\endfoot

\endlastfoot

{\hyperref[\detokenize{keywords/index:add-fixed-length-schedule-kw}]{\sphinxcrossref{\DUrole{std,std-ref}{ADD\_FIXED\_LENGTH\_SCHEDULE\_KW}}}}
&
NO
&&
Supporting unknown SCHEDULE keywords.
\\
\hline
{\hyperref[\detokenize{keywords/index:analysis-copy}]{\sphinxcrossref{\DUrole{std,std-ref}{ANALYSIS\_COPY}}}}
&
NO
&&
Create new instance of analysis module
\\
\hline
{\hyperref[\detokenize{keywords/index:analysis-load}]{\sphinxcrossref{\DUrole{std,std-ref}{ANALYSIS\_LOAD}}}}
&
NO
&&
Load analysis module
\\
\hline
{\hyperref[\detokenize{keywords/index:analysis-set-var}]{\sphinxcrossref{\DUrole{std,std-ref}{ANALYSIS\_SET\_VAR}}}}
&
NO
&&
Set analysis module internal state variable
\\
\hline
{\hyperref[\detokenize{keywords/index:analysis-select}]{\sphinxcrossref{\DUrole{std,std-ref}{ANALYSIS\_SELECT}}}}
&
NO
&
STD\_ENKF
&
Select analysis module to use in update
\\
\hline
\DUrole{xref,std,std-ref}{CASE\_TABLE}
&
NO
&&
For running sensitivities you can give the cases descriptive names
\\
\hline
\DUrole{xref,std,std-ref}{CONTAINER}
&
NO
&&
…
\\
\hline
{\hyperref[\detokenize{keywords/index:custom-kw}]{\sphinxcrossref{\DUrole{std,std-ref}{CUSTOM\_KW}}}}
&
NO
&&
Ability to load arbitrary values from the forward model.
\\
\hline
{\hyperref[\detokenize{keywords/index:data-file}]{\sphinxcrossref{\DUrole{std,std-ref}{DATA\_FILE}}}}
&
YES
&&
Provide an ECLIPSE data file for the problem.
\\
\hline
{\hyperref[\detokenize{keywords/index:data-kw}]{\sphinxcrossref{\DUrole{std,std-ref}{DATA\_KW}}}}
&
NO
&&
Replace strings in ECLIPSE .DATA files
\\
\hline
\DUrole{xref,std,std-ref}{DBASE\_TYPE}
&
NO
&
BLOCK\_FS
&
Which ‘database’ system should be used for storage
\\
\hline
{\hyperref[\detokenize{keywords/index:define}]{\sphinxcrossref{\DUrole{std,std-ref}{DEFINE}}}}
&
NO
&&
Define keywords with config scope
\\
\hline
{\hyperref[\detokenize{keywords/index:delete-runpath}]{\sphinxcrossref{\DUrole{std,std-ref}{DELETE\_RUNPATH}}}}
&
NO
&&
Explicitly tell ert to delete the runpath when a job is complete
\\
\hline
{\hyperref[\detokenize{keywords/index:eclbase}]{\sphinxcrossref{\DUrole{std,std-ref}{ECLBASE}}}}
&
YES
&&
Define a name for the ECLIPSE simulations.
\\
\hline
{\hyperref[\detokenize{keywords/index:end-date}]{\sphinxcrossref{\DUrole{std,std-ref}{END\_DATE}}}}
&
NO
&&
You can tell ERT how lon the simulations should be - for error check
\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-alpha}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_ALPHA}}}}
&
NO
&
1.50
&
Parameter controlling outlier behaviour in EnKF algorithm
\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-bootstrap}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_BOOTSTRAP}}}}
&
NO
&
FALSE
&
Should we bootstrap the Kalman gain estimate
\\
\hline
\DUrole{xref,std,std-ref}{ENKF\_CROSS\_VALIDATION}
&
NO
&
…
&\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-cv-folds}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_CV\_FOLDS}}}}
&
NO
&
10
&
Number of folds used in the Cross-Validation scheme
\\
\hline
\DUrole{xref,std,std-ref}{ENKF\_KERNEL\_PARAM}
&
NO
&
1
&\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-local-cv}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_LOCAL\_CV}}}}
&
NO
&
FALSE
&
Should we estimate the subspace dimenseion using Cross-Validation
\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-merge-observations}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_MERGE\_OBSERVATIONS}}}}
&
NO
&
FALSE
&
Should observations from many times be merged together
\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-mode}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_MODE}}}}
&
NO
&
STANDARD
&
Which EnKF should be used
\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-pen-press}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_PEN\_PRESS}}}}
&
NO
&
FALSE
&
Should we want to use a penalised PRESS statistic in model selection?
\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-rerun}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_RERUN}}}}
&
NO
&
FALSE
&
Should the simulations be restarted from time zero after each update.
\\
\hline
{\hyperref[\detokenize{keywords/index:enkf-scaling}]{\sphinxcrossref{\DUrole{std,std-ref}{ENKF\_SCALING}}}}
&
NO
&
TRUE
&
Do we want to normalize the data ensemble to have unit variance?
\\
\hline
\DUrole{xref,std,std-ref}{ENKF\_TRUNCATION}
&
NO
&
0.99
&
Cutoff used on singular value spectrum.
\\
\hline
{\hyperref[\detokenize{keywords/index:enspath}]{\sphinxcrossref{\DUrole{std,std-ref}{ENSPATH}}}}
&
NO
&
storage
&
Folder used for storage of simulation results.
\\
\hline
{\hyperref[\detokenize{keywords/index:field}]{\sphinxcrossref{\DUrole{std,std-ref}{FIELD}}}}
&
NO
&&
Ads grid parameters
\\
\hline
{\hyperref[\detokenize{keywords/index:forward-model}]{\sphinxcrossref{\DUrole{std,std-ref}{FORWARD\_MODEL}}}}
&
NO
&&
Add the running of a job to the simulation forward model.
\\
\hline
{\hyperref[\detokenize{keywords/index:gen-data}]{\sphinxcrossref{\DUrole{std,std-ref}{GEN\_DATA}}}}
&
NO
&&
Specify a general type of data created/updated by the forward model.
\\
\hline
{\hyperref[\detokenize{keywords/index:gen-kw}]{\sphinxcrossref{\DUrole{std,std-ref}{GEN\_KW}}}}
&
NO
&&
Add a scalar parameter.
\\
\hline
\DUrole{xref,std,std-ref}{GEN\_KW\_TAG\_FORMAT}
&
NO
&
\textless{}\%s\textgreater{}
&
Format used to add keys in the GEN\_KW template files.
\\
\hline
\DUrole{xref,std,std-ref}{GEN\_KW\_EXPORT\_FILE}
&
NO
&
parameter.txt
&
Name of file to export GEN\_KW parameters to.
\\
\hline
{\hyperref[\detokenize{keywords/index:gen-param}]{\sphinxcrossref{\DUrole{std,std-ref}{GEN\_PARAM}}}}
&
NO
&&
Add a general parameter.
\\
\hline
{\hyperref[\detokenize{keywords/index:grid}]{\sphinxcrossref{\DUrole{std,std-ref}{GRID}}}}
&
NO
&&
Provide an ECLIPSE grid for the reservoir model.
\\
\hline
{\hyperref[\detokenize{keywords/index:history-source}]{\sphinxcrossref{\DUrole{std,std-ref}{HISTORY\_SOURCE}}}}
&
NO
&
REFCASE\_HISTORY
&
Source used for historical values.
\\
\hline
{\hyperref[\detokenize{keywords/index:hook-workflow}]{\sphinxcrossref{\DUrole{std,std-ref}{HOOK\_WORKFLOW}}}}
&
NO
&&
Install a workflow to be run automatically.
\\
\hline
\DUrole{xref,std,std-ref}{IGNORE\_SCHEDULE}
&
NO
&&\\
\hline
\DUrole{xref,std,std-ref}{INSTALL\_JOB}
&
NO
&&
Install a job for use in a forward model.
\\
\hline
\DUrole{xref,std,std-ref}{ITER\_CASE}
&
NO
&
IES\%d
&
Case name format - iterated ensemble smoother
\\
\hline
\DUrole{xref,std,std-ref}{ITER\_COUNT}
&
NO
&
4
&
Number of iterations - iterated ensemble smoother
\\
\hline
\DUrole{xref,std,std-ref}{ITER\_RETRY\_COUNT}
&
NO
&
4
&
Number of retries for a iteration - iterated ensemble smoother
\\
\hline
{\hyperref[\detokenize{keywords/index:jobname}]{\sphinxcrossref{\DUrole{std,std-ref}{JOBNAME}}}}
&
NO
&&
Name used for simulation files. An alternative to ECLBASE.
\\
\hline
{\hyperref[\detokenize{keywords/index:job-script}]{\sphinxcrossref{\DUrole{std,std-ref}{JOB\_SCRIPT}}}}
&
NO
&&
Python script managing the forward model.
\\
\hline
\DUrole{xref,std,std-ref}{LOAD\_SEED}
&
NO
&&
Load random seed from given file.
\\
\hline
\DUrole{xref,std,std-ref}{LOAD\_WORKFLOW}
&
NO
&&
Load a workflow into ERT.
\\
\hline
\DUrole{xref,std,std-ref}{LOAD\_WORKFLOW\_JOB}
&
NO
&&
Load a workflow job into ERT.
\\
\hline
\DUrole{xref,std,std-ref}{LICENSE\_PATH}
&
NO
&&
A path where ert-licenses to e.g. RMS are stored.
\\
\hline
\DUrole{xref,std,std-ref}{LOCAL\_CONFIG}
&
NO
&&
A file with configuration information for local analysis.
\\
\hline
\DUrole{xref,std,std-ref}{LOG\_FILE}
&
NO
&
log
&
Name of log file
\\
\hline
\DUrole{xref,std,std-ref}{LOG\_LEVEL}
&
NO
&
1
&
How much logging?
\\
\hline
{\hyperref[\detokenize{keywords/index:lsf-queue}]{\sphinxcrossref{\DUrole{std,std-ref}{LSF\_QUEUE}}}}
&
NO
&
normal
&
Name of LSF queue.
\\
\hline
\DUrole{xref,std,std-ref}{LSF\_RESOURCES}
&
NO
&&\\
\hline
{\hyperref[\detokenize{keywords/index:lsf-server}]{\sphinxcrossref{\DUrole{std,std-ref}{LSF\_SERVER}}}}
&
NO
&&
Set server used when submitting LSF jobs.
\\
\hline
\DUrole{xref,std,std-ref}{MAX\_ITER\_COUNT}
&
NO
&&
Maximal number of iterations - iterated ensemble smoother.
\\
\hline
\DUrole{xref,std,std-ref}{MAX\_RESAMPLE}
&
NO
&
1
&
How many times should ert resample \& retry a simulation.
\\
\hline
\DUrole{xref,std,std-ref}{MAX\_RUNNING\_RSH}
&
NO
&&
The maximum number of running jobs when using RSH queue system.
\\
\hline
{\hyperref[\detokenize{keywords/index:max-runtime}]{\sphinxcrossref{\DUrole{std,std-ref}{MAX\_RUNTIME}}}}
&
NO
&
0
&
Set the maximum runtime in seconds for a realization.
\\
\hline
\DUrole{xref,std,std-ref}{MAX\_SUBMIT}
&
NO
&
2
&
How many times should the queue system retry a simulation.
\\
\hline
{\hyperref[\detokenize{keywords/index:min-realizations}]{\sphinxcrossref{\DUrole{std,std-ref}{MIN\_REALIZATIONS}}}}
&
NO
&
0
&
Set the number of minimum reservoir realizations to run before long running realizations are stopped. Keyword STOP\_LONG\_RUNNING must be set to TRUE when MIN\_REALIZATIONS are set.
\\
\hline
{\hyperref[\detokenize{keywords/index:num-realizations}]{\sphinxcrossref{\DUrole{std,std-ref}{NUM\_REALIZATIONS}}}}
&
YES
&&
Set the number of reservoir realizations to use.
\\
\hline
{\hyperref[\detokenize{keywords/index:obs-config}]{\sphinxcrossref{\DUrole{std,std-ref}{OBS\_CONFIG}}}}
&
NO
&&
File specifying observations with uncertainties.
\\
\hline
{\hyperref[\detokenize{keywords/index:plot-driver}]{\sphinxcrossref{\DUrole{std,std-ref}{PLOT\_SETTINGS}}}}
&
NO
&&
Possibility to configure some aspects of plotting.
\\
\hline
\DUrole{xref,std,std-ref}{PRE\_CLEAR\_RUNPATH}
&
NO
&
FALSE
&
Should the runpath be cleared before initializing?
\\
\hline
{\hyperref[\detokenize{keywords/index:queue-system}]{\sphinxcrossref{\DUrole{std,std-ref}{QUEUE\_SYSTEM}}}}
&
NO
&&
System used for running simulation jobs.
\\
\hline
{\hyperref[\detokenize{keywords/index:refcase}]{\sphinxcrossref{\DUrole{std,std-ref}{REFCASE}}}}
&
NO (see HISTORY\_SOURCE and SUMMARY)
&&
Reference case used for observations and plotting.
\\
\hline
\DUrole{xref,std,std-ref}{REFCASE\_LIST}
&
NO
&&
Full path to Eclipse .DATA files containing completed runs (which you can add to plots)
\\
\hline
\DUrole{xref,std,std-ref}{RERUN\_PATH}
&
NO
&&
…
\\
\hline
\DUrole{xref,std,std-ref}{RERUN\_START}
&
NO
&
0
&
…
\\
\hline
{\hyperref[\detokenize{keywords/index:rft-config}]{\sphinxcrossref{\DUrole{std,std-ref}{RFT\_CONFIG}}}}
&
NO
&&
Config file specifying wellnames and dates for rft-measurments. Used for plotting. The format has to be name day month year (ex. Q-2FI 02 08 1973), with a new entry on a new line.
\\
\hline
{\hyperref[\detokenize{keywords/index:rftpath}]{\sphinxcrossref{\DUrole{std,std-ref}{RFTPATH}}}}
&
NO
&
rft
&
Path to where the rft well observations are stored
\\
\hline
{\hyperref[\detokenize{keywords/index:rsh-command}]{\sphinxcrossref{\DUrole{std,std-ref}{RSH\_COMMAND}}}}
&
NO
&&
Command used for remote shell operations.
\\
\hline
{\hyperref[\detokenize{keywords/index:rsh-host}]{\sphinxcrossref{\DUrole{std,std-ref}{RSH\_HOST}}}}
&
NO
&&
Remote host used to run forward model.
\\
\hline
\DUrole{xref,std,std-ref}{RUNPATH}
&
NO
&
simulations/realization\%d
&
Directory to run simulations
\\
\hline
\DUrole{xref,std,std-ref}{RUN\_TEMPLATE}
&
NO
&&
Install arbitrary files in the runpath directory.
\\
\hline
{\hyperref[\detokenize{keywords/index:std-scale-correlated-obs}]{\sphinxcrossref{\DUrole{std,std-ref}{STD\_SCALE\_CORRELATED\_OBS}}}}
&
NO
&
FALSE
&
Try to estimate the correlations in the data to inflate the observation std.
\\
\hline
{\hyperref[\detokenize{keywords/index:schedule-file}]{\sphinxcrossref{\DUrole{std,std-ref}{SCHEDULE\_FILE}}}}
&
NO
&&
Provide an ECLIPSE schedule file for the problem.
\\
\hline
{\hyperref[\detokenize{keywords/index:schedule-prediction-file}]{\sphinxcrossref{\DUrole{std,std-ref}{SCHEDULE\_PREDICTION\_FILE}}}}
&
NO
&&
Schedule prediction file.
\\
\hline
{\hyperref[\detokenize{keywords/index:setenv}]{\sphinxcrossref{\DUrole{std,std-ref}{SETENV}}}}
&
NO
&&
You can modify the UNIX environment with SETENV calls.
\\
\hline
\DUrole{xref,std,std-ref}{SINGLE\_NODE\_UPDATE}
&
NO
&
FALSE
&
…
\\
\hline
{\hyperref[\detokenize{keywords/index:stop-long-running}]{\sphinxcrossref{\DUrole{std,std-ref}{STOP\_LONG\_RUNNING}}}}
&
NO
&
FALSE
&
Stop long running realizations after minimum number of realizations (MIN\_REALIZATIONS) have run.
\\
\hline
\DUrole{xref,std,std-ref}{STORE\_SEED}
&
NO
&&
File where the random seed used is stored.
\\
\hline
{\hyperref[\detokenize{keywords/index:summary}]{\sphinxcrossref{\DUrole{std,std-ref}{SUMMARY}}}}
&
NO
&&
Add summary variables for internalization.
\\
\hline
{\hyperref[\detokenize{keywords/index:surface}]{\sphinxcrossref{\DUrole{std,std-ref}{SURFACE}}}}
&
NO
&&
Surface parameter read from RMS IRAP file.
\\
\hline
\DUrole{xref,std,std-ref}{TORQUE\_QUEUE}
&
NO
&&
…
\\
\hline
{\hyperref[\detokenize{keywords/index:time-map}]{\sphinxcrossref{\DUrole{std,std-ref}{TIME\_MAP}}}}
&
NO
&&
Ability to manually enter a list of dates to establish report step \textless{}-\textgreater{} dates mapping.
\\
\hline
{\hyperref[\detokenize{keywords/index:umask}]{\sphinxcrossref{\DUrole{std,std-ref}{UMASK}}}}
&
NO
&&
Control the permissions on files created by ERT.
\\
\hline
{\hyperref[\detokenize{keywords/index:update-log-path}]{\sphinxcrossref{\DUrole{std,std-ref}{UPDATE\_LOG\_PATH}}}}
&
NO
&
update\_log
&
Summary of the EnKF update steps are stored in this directory.
\\
\hline
{\hyperref[\detokenize{keywords/index:update-path}]{\sphinxcrossref{\DUrole{std,std-ref}{UPDATE\_PATH}}}}
&
NO
&&
Modify a UNIX path variable like LD\_LIBRARY\_PATH.
\\
\hline
{\hyperref[\detokenize{keywords/index:update-settings}]{\sphinxcrossref{\DUrole{std,std-ref}{UPDATE\_SETTINGS}}}}
&
NO
&&
Possibility to configure some common aspects of the Smoother update.\textbar{}
\\
\hline
\DUrole{xref,std,std-ref}{WORKFLOW\_JOB\_DIRECTORY}
&
NO
&&
Directory containing workflow jobs.
\\
\hline
\end{longtable}\sphinxatlongtableend\end{savenotes}

:ref:{}` \textless{}\textgreater{}{}`


\section{Basic required keywords}
\label{\detokenize{keywords/index:basic-required-keywords}}\phantomsection\label{\detokenize{keywords/index:id1}}
These keywords must be set to make the enkf function properly.

\phantomsection\label{\detokenize{keywords/index:data-file}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{DATA\_FILE}

\begin{DUlineblock}{0em}
\item[] This is the name of ECLIPSE data file used to control the simulations. The
\end{DUlineblock}

data file should be prepared according to the guidelines given in Preparing an
ECLIPSE reservoir model for use with enkf.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Load} \PYG{n}{the} \PYG{n}{data} \PYG{n}{file} \PYG{n}{called} \PYG{n}{ECLIPSE}\PYG{o}{.}\PYG{n}{DATA}
\PYG{n}{DATA\PYGZus{}FILE} \PYG{n}{ECLIPSE}\PYG{o}{.}\PYG{n}{DATA}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:eclbase}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ECLBASE}

\begin{DUlineblock}{0em}
\item[] The ECLBASE keyword sets the basename used for the ECLIPSE simulations. It
\end{DUlineblock}

can (and should, for your convenience) contain a \%d specifier, which will be
replaced with the realization numbers when running ECLIPSE. Note that due to
limitations in ECLIPSE, the ECLBASE string must be in strictly upper or lower
case.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Use} \PYG{n}{MY\PYGZus{}VERY\PYGZus{}OWN\PYGZus{}OIL\PYGZus{}FIELD}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{0} \PYG{n}{etc}\PYG{o}{.} \PYG{k}{as} \PYG{n}{basename}\PYG{o}{.}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{When} \PYG{n}{ECLIPSE} \PYG{o+ow}{is} \PYG{n}{running}\PYG{p}{,} \PYG{n}{the} \PYG{o}{\PYGZpc{}}\PYG{n}{d} \PYG{n}{will} \PYG{n}{be}\PYG{p}{,}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{replaced} \PYG{k}{with} \PYG{n}{realization} \PYG{n}{number}\PYG{p}{,} \PYG{n}{giving}\PYG{p}{:}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{MY\PYGZus{}VERY\PYGZus{}OWN\PYGZus{}OIL\PYGZus{}FIELD}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{0}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{MY\PYGZus{}VERY\PYGZus{}OWN\PYGZus{}OIL\PYGZus{}FIELD}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{MY\PYGZus{}VERY\PYGZus{}OWN\PYGZus{}OIL\PYGZus{}FIELD}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{o+ow}{and} \PYG{n}{so} \PYG{n}{on}\PYG{o}{.}
\PYG{n}{ECLBASE} \PYG{n}{MY\PYGZus{}VERY\PYGZus{}OWN\PYGZus{}OIL\PYGZus{}FIELD}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZpc{}}\PYG{n}{d}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:jobname}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{JOBNAME}

As an alternative to the ECLBASE keyword you can use the JOBNAME keyword; in
particular in cases where your forward model does not include ECLIPSE at all
that makes more sense. If JOBANME is used instead of ECLBASE the same rules of
no-mixed-case apply.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:grid}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{GRID}

This is the name of an existing GRID/EGRID file for your ECLIPSE model. If you
had to create a new grid file when preparing your ECLIPSE reservoir model for
use with enkf, this should point to the new .EGRID file.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Load} \PYG{n}{the} \PYG{o}{.}\PYG{n}{EGRID} \PYG{n}{file} \PYG{n}{called} \PYG{n}{MY\PYGZus{}GRID}\PYG{o}{.}\PYG{n}{EGRID}
\PYG{n}{GRID} \PYG{n}{MY\PYGZus{}GRID}\PYG{o}{.}\PYG{n}{EGRID}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:num-realizations}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{NUM\_REALIZATIONS}

This is just the size of the ensemble, i.e. the number of realizations/members
in the ensemble.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Use} \PYG{l+m+mi}{200} \PYG{n}{realizations}\PYG{o}{/}\PYG{n}{members}
\PYG{n}{NUM\PYGZus{}REALIZATIONS} \PYG{l+m+mi}{200}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:schedule-file}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{SCHEDULE\_FILE}

This keyword should be the name a text file containing the SCHEDULE section of
the ECLIPSE data file. It should be prepared in accordance with the guidelines
given in Preparing an ECLIPSE reservoir model for use with enkf. This SCHEDULE
section will be used to control the ECLIPSE simulations. You can optionally
give a second filename, which is the name of file which will be written into
the directories for running ECLIPSE.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Parse} \PYG{n}{MY\PYGZus{}SCHEDULE}\PYG{o}{.}\PYG{n}{SCH}\PYG{p}{,} \PYG{n}{call} \PYG{n}{the} \PYG{n}{generated} \PYG{n}{file} \PYG{n}{ECLIPSE\PYGZus{}SCHEDULE}\PYG{o}{.}\PYG{n}{SCH}
\PYG{n}{SCHEDULE\PYGZus{}FILE} \PYG{n}{MY\PYGZus{}SCHEDULE}\PYG{o}{.}\PYG{n}{SCH} \PYG{n}{ECLIPSE\PYGZus{}SCHEDULE}\PYG{o}{.}\PYG{n}{SCH}
\end{sphinxVerbatim}

Observe that the SCHEDULE\_FILE keyword is only required when you need ERT to
stop and restart your simulations; i.e. when you are using the EnKF algorithm.
If you are only using ERT to your simulations; or using smoother update it is
recommended to leave the SCHEDULE\_FILE keyword out. In that case you must make
sure that the ECLIPSE datafile correctly includes the SCHEDULE section.
\end{sphinxShadowBox}


\section{Basic optional keywords}
\label{\detokenize{keywords/index:basic-optional-keywords}}\phantomsection\label{\detokenize{keywords/index:id2}}
These keywords are optional. However, they serve many useful purposes, and it is
recommended that you read through this section to get a thorough idea of what’s
possible to do with the enkf application.

\phantomsection\label{\detokenize{keywords/index:data-kw}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{DATA\_KW}

The keyword DATA\_KW can be used for inserting strings into placeholders in the
ECLIPSE data file. For instance, it can be used to insert include paths.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Define} \PYG{n}{the} \PYG{n}{alias} \PYG{n}{MY\PYGZus{}PATH} \PYG{n}{using} \PYG{n}{DATA\PYGZus{}KW}\PYG{o}{.} \PYG{n}{Any} \PYG{n}{instances} \PYG{n}{of} \PYG{o}{\PYGZlt{}}\PYG{n}{MY\PYGZus{}PATH}\PYG{o}{\PYGZgt{}} \PYG{p}{(}\PYG{n}{yes}\PYG{p}{,} \PYG{k}{with} \PYG{n}{brackets}\PYG{p}{)}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{o+ow}{in} \PYG{n}{the} \PYG{n}{ECLIPSE} \PYG{n}{data} \PYG{n}{file} \PYG{n}{will} \PYG{n}{now} \PYG{n}{be} \PYG{n}{replaced} \PYG{k}{with} \PYG{o}{/}\PYG{n}{mnt}\PYG{o}{/}\PYG{n}{my\PYGZus{}own\PYGZus{}disk}\PYG{o}{/}\PYG{n}{my\PYGZus{}reservoir\PYGZus{}model}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{when} \PYG{n}{running} \PYG{n}{the} \PYG{n}{ECLIPSE} \PYG{n}{jobs}\PYG{o}{.}
\PYG{n}{DATA\PYGZus{}KW}  \PYG{n}{MY\PYGZus{}PATH}  \PYG{o}{/}\PYG{n}{mnt}\PYG{o}{/}\PYG{n}{my\PYGZus{}own\PYGZus{}disk}\PYG{o}{/}\PYG{n}{my\PYGZus{}reservoir\PYGZus{}model}
\end{sphinxVerbatim}

The DATA\_KW keyword is of course optional. Note also that the enkf has some
built in magic strings.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:delete-runpath}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{DELETE\_RUNPATH}

When the ert application is running it creates directories for
the forward model simulations, one for each realization. When
the simulations are done, ert will load the results into the
internal database. By default the realization folders will be
left intact after ert has loaded the results, but using the
keyword DELETE\_RUNPATH you can request to have (some of) the
directories deleted after results have been loaded.

\sphinxstyleemphasis{Example A:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Delete} \PYG{n}{simulation} \PYG{n}{directories} \PYG{l+m+mi}{0} \PYG{n}{to} \PYG{l+m+mi}{99}
\PYG{n}{DELETE\PYGZus{}RUNPATH} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{99}
\end{sphinxVerbatim}

\sphinxstyleemphasis{Example B:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Delete} \PYG{n}{simulation} \PYG{n}{directories} \PYG{l+m+mi}{0} \PYG{n}{to} \PYG{l+m+mi}{10} \PYG{k}{as} \PYG{n}{well} \PYG{k}{as} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{15} \PYG{o+ow}{and} \PYG{l+m+mf}{20.}
\PYG{n}{DELETE\PYGZus{}RUNPATH} \PYG{l+m+mi}{0} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{20}
\end{sphinxVerbatim}

The DELETE\_RUNPATH keyword is optional.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:end-date}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{END\_DATE}

When running a set of models from beginning to end ERT does
not now in advance how long the simulation is supposed to be,
it is therefor impossible beforehand to determine which
restart file number should be used as target file, and the
procedure used for EnKF runs can not be used to verify that an
ECLIPSE simulation has run to the end.

By using the END\_DATE keyword you can tell ERT that the
simulation should go at least up to the date given by
END\_DATE, otherwise they will be regarded as failed. The
END\_DATE does not need to correspond exactly to the end date
of the simulation, it must just be set so that all simulations
which go to or beyond END\_DATE are regarded as successfull.

\sphinxstyleemphasis{Example:}
\begin{description}
\item[{::}] \leavevmode
END\_DATE  10/10/2010

\end{description}

With this END\_DATE setting all simulations which have gone to
at least 10.th of October 2010 are OK.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enspath}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENSPATH}

The ENSPATH should give the name of a folder that will be used
for storage by the enkf application. Note that the contents of
this folder is not intended for human inspection. By default,
ENSPATH is set to “storage”.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Use} \PYG{n}{internal} \PYG{n}{storage} \PYG{o+ow}{in} \PYG{o}{/}\PYG{n}{mnt}\PYG{o}{/}\PYG{n}{my\PYGZus{}big\PYGZus{}enkf\PYGZus{}disk}
\PYG{n}{ENSPATH} \PYG{o}{/}\PYG{n}{mnt}\PYG{o}{/}\PYG{n}{my\PYGZus{}big\PYGZus{}enkf\PYGZus{}disk}
\end{sphinxVerbatim}

The ENSPATH keyword is optional.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:history-source}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{HISTORY\_SOURCE}

In the observation configuration file you can enter
observations with the keyword HISTORY\_OBSERVATION; this means
that ERT will the observed ‘true’ values from the model
history. Practically the historical values can be fetched
either from the SCHEDULE file or from a reference case. What
source to use for the historical values can be controlled with
the HISTORY\_SOURCE keyword. The different possible values for
the HISTORY\_SOURCE keyword are:
\begin{description}
\item[{REFCASE\_HISTORY}] \leavevmode
This is the default value for HISTORY\_SOURCE,
ERT will fetch the historical values from the \sphinxstyleemphasis{xxxH}
keywords in the refcase summary, e.g. observations of
WGOR:OP\_1 is based the WGORH:OP\_1 vector from the
refcase summary.

\item[{REFCASE\_SIMULATED In this case the historical values are based on the}] \leavevmode
simulated values from the refcase, this is mostly relevant when a you want
compare with another case which serves as ‘the truth’.

\item[{SCHEDULE Load historical values from the WCONHIST and WCONINJE keywords in the}] \leavevmode
Schedule file.

\end{description}

When setting HISTORY\_SOURCE to either REFCASE\_SIMULATED or REFCASE\_HISTORY you
must also set the REFCASE variable to point to the ECLIPSE data file in an
existing reference case (should be created with the same schedule file as you
are using now).

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Use} \PYG{n}{historic} \PYG{n}{data} \PYG{k+kn}{from} \PYG{n+nn}{reference} \PYG{n}{case}
\PYG{n}{HISTORY\PYGZus{}SOURCE}  \PYG{n}{REFCASE\PYGZus{}HISTORY}
\PYG{n}{REFCASE}         \PYG{o}{/}\PYG{n}{somefolder}\PYG{o}{/}\PYG{n}{ECLIPSE}\PYG{o}{.}\PYG{n}{DATA}
\end{sphinxVerbatim}

The HISTORY\_SOURCE keyword is optional.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:refcase}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{REFCASE}

With the REFCASE key you can supply ert with a reference case which can be
used for observations (see HISTORY\_SOURCE), if you want to use wildcards with
the SUMMARY keyword you also must supply a REFCASE keyword. The REFCASE
keyword should just point to an existing ECLIPSE data file; ert will then look
up and load the corresponding summary results.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{The} \PYG{n}{REFCASE} \PYG{n}{keyword} \PYG{n}{points} \PYG{n}{to} \PYG{n}{the} \PYG{n}{datafile} \PYG{n}{of} \PYG{n}{an} \PYG{n}{existing} \PYG{n}{ECLIPSE} \PYG{n}{simulation}\PYG{o}{.}
\PYG{n}{REFCASE} \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{somewhere}\PYG{o}{/}\PYG{n}{SIM\PYGZus{}01\PYGZus{}BASE}\PYG{o}{.}\PYG{n}{DATA}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:install-job}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{INSTALL\_JOB}

The INSTALL\_JOB keyword is used to learn the enkf application how to run
external applications and scripts, i.e. defining a job. After a job has been
defined with INSTALL\_JOB, it can be used with the FORWARD\_MODEL keyword. For
example, if you have a script which generates relative permeability curves
from a set of parameters, it can be added as a job, allowing you to do history
matching and sensitivity analysis on the parameters defining the relative
permeability curves.

The INSTALL\_JOB keyword takes two arguments, a job name and the name of a
configuration file for that particular job.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Define} \PYG{n}{a} \PYG{n}{Lomeland} \PYG{n}{relative} \PYG{n}{permeabilty} \PYG{n}{job}\PYG{o}{.}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{The} \PYG{n}{file} \PYG{n}{jobs}\PYG{o}{/}\PYG{n}{lomeland}\PYG{o}{.}\PYG{n}{txt} \PYG{n}{contains} \PYG{n}{a} \PYG{n}{detailed}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{specification} \PYG{n}{of} \PYG{n}{the} \PYG{n}{job}\PYG{o}{.}
\PYG{n}{INSTALL\PYGZus{}JOB} \PYG{n}{LOMELAND} \PYG{n}{jobs}\PYG{o}{/}\PYG{n}{lomeland}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

The configuration file used to specify an external job is easy to use and very
flexible. It is documented in Customizing the simulation workflow in enkf.

The INSTALL\_JOB keyword is optional.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:obs-config}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{OBS\_CONFIG}

The OBS\_CONFIG key should point to a file defining observations and associated
uncertainties. The file should be in plain text and formatted according to the
guidelines given in Creating an observation file for use with enkf.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Use} \PYG{n}{the} \PYG{n}{observations} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}observations}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{OBS\PYGZus{}CONFIG} \PYG{n}{my\PYGZus{}observations}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

The OBS\_CONFIG keyword is optional, but for your own convenience, it is
strongly recommended to provide an observation file.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:result-path}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{RESULT\_PATH}

The enkf application will print some simple tabulated results at each report
step. The RESULT\_PATH keyword should point to a folder where the tabulated
results are to be written. It can contain a \%d specifier, which will be
replaced with the report step by enkf. The default value for RESULT\_PATH is
“results/step\_\%d”.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Changing} \PYG{n}{RESULT\PYGZus{}PATH}
\PYG{n}{RESULT\PYGZus{}PATH} \PYG{n}{my\PYGZus{}nice\PYGZus{}results}\PYG{o}{/}\PYG{n}{step}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZpc{}}\PYG{n}{d}
\end{sphinxVerbatim}

The RESULT\_PATH keyword is optional.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:runpath}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{RUNPATH}

The RUNPATH keyword should give the name of the folders where the ECLIPSE
simulations are executed. It should contain at least one \%d specifier, which
will be replaced by the realization number when the enkf creates the folders.
Optionally, it can contain one more \%d specifier, which will be replaced by
the iteration number.

By default, RUNPATH is set to “simulations/realization-\%d”.

\sphinxstyleemphasis{Example A:}
\begin{description}
\item[{::}] \leavevmode
\textendash{} Giving a RUNPATH with just one \%d specifer.
RUNPATH /mnt/my\_scratch\_disk/realization-\%d

\end{description}

\sphinxstyleemphasis{Example B:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Giving} \PYG{n}{a} \PYG{n}{RUNPATH} \PYG{k}{with} \PYG{n}{two} \PYG{o}{\PYGZpc{}}\PYG{n}{d} \PYG{n}{specifers}\PYG{o}{.}
\PYG{n}{RUNPATH} \PYG{o}{/}\PYG{n}{mnt}\PYG{o}{/}\PYG{n}{my\PYGZus{}scratch\PYGZus{}disk}\PYG{o}{/}\PYG{n}{realization}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZpc{}}\PYG{n}{d}\PYG{o}{/}\PYG{n}{iteration}\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZpc{}}\PYG{n}{d}
\end{sphinxVerbatim}

The RUNPATH keyword is optional.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:runpath-file}}
When running workflows based on external scripts it is necessary to ‘tell’ the
external script in some way or another were all the realisations are located in
the filesystem. Since the number of realisations can be quite high this will
easily overflow the commandline buffer; the solution which is used is therefor
to let ert write a reagular file which looks like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{0}   \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{realisation0}   \PYG{n}{CASE0}   \PYG{n+nb}{iter}
\PYG{l+m+mi}{1}   \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{realisation1}   \PYG{n}{CASE1}   \PYG{n+nb}{iter}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{n}{N}   \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{realisationN}   \PYG{n}{CASEN}   \PYG{n+nb}{iter}
\end{sphinxVerbatim}

The path to this file can then be passed to the scripts using the
magic string \textless{}RUNPATH\_FILE\textgreater{}. The RUNPATH\_FILE will by default be
stored as .ert\_runpath\_list in the same directory as the configuration
file, but you can set it to something else with the RUNPATH\_FILE key.


\section{Keywords controlling the simulations}
\label{\detokenize{keywords/index:keywords-controlling-the-simulations}}\phantomsection\label{\detokenize{keywords/index:id3}}\phantomsection\label{\detokenize{keywords/index:min-realizations}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{MIN\_REALIZATIONS}

MIN\_REALIZATIONS is the minimum number of realizations that
must have succeeded for the simulation to be regarded as a
success.

MIN\_REALIZATIONS can also be used in combination with
STOP\_LONG\_RUNNING, see the documentation for STOP\_LONG\_RUNNING
for a description of this.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{MIN\PYGZus{}REALIZATIONS}  \PYG{l+m+mi}{20}
\end{sphinxVerbatim}

The MIN\_REALIZATIONS key can also be set as a percentage of
NUM\_REALIZATIONS

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{MIN\PYGZus{}REALIZATIONS}  \PYG{l+m+mi}{10}\PYG{o}{\PYGZpc{}}
\end{sphinxVerbatim}

The MIN\_REALIZATIONS key is optional, but if it has not been
set \sphinxstyleemphasis{all} the realisations must succeed.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:stop-long-running}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{STOP\_LONG\_RUNNING}

The STOP\_LONG\_RUNNING key is used in combination with the MIN\_REALIZATIONS key
to control the runtime of simulations. When STOP\_LONG\_RUNNING is set to TRUE,
MIN\_REALIZATIONS is the minimum number of realizations run before the
simulation is stopped. After MIN\_REALIZATIONS have succeded successfully, the
realizatons left are allowed to run for 25\% of the average runtime for
successfull realizations, and then killed.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Stop} \PYG{n}{long} \PYG{n}{running} \PYG{n}{realizations} \PYG{n}{after} \PYG{l+m+mi}{20} \PYG{n}{realizations} \PYG{n}{have} \PYG{n}{succeeded}
\PYG{n}{MIN\PYGZus{}REALIZATIONS}  \PYG{l+m+mi}{20}
\PYG{n}{STOP\PYGZus{}LONG\PYGZus{}RUNNING} \PYG{n}{TRUE}
\end{sphinxVerbatim}

The STOP\_LONG\_RUNNING key is optional. The MIN\_REALIZATIONS key must be set
when STOP\_LONG\_RUNNING is set to TRUE.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:max-runtime}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{MAX\_RUNTIME}

The MAX\_RUNTIME keyword is used to control the runtime of simulations. When
MAX\_RUNTIME is set, a job is only allowed to run for MAX\_RUNTIME, given in
seconds. A value of 0 means unlimited runtime.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Let} \PYG{n}{each} \PYG{n}{realizations} \PYG{n}{run} \PYG{k}{for} \PYG{l+m+mi}{50} \PYG{n}{seconds}
\PYG{n}{MAX\PYGZus{}RUNTIME} \PYG{l+m+mi}{50}
\end{sphinxVerbatim}

The MAX\_RUNTIME key is optional.
\end{sphinxShadowBox}


\section{Parameterization keywords}
\label{\detokenize{keywords/index:parameterization-keywords}}\phantomsection\label{\detokenize{keywords/index:id4}}
The keywords in this section are used to define a parametrization of the ECLIPSE
model. I.e., defining which parameters to change in a sensitivity analysis
and/or history matching project. For some parameters, it necessary to specify a
prior distribution. See Prior distributions available in enkf for a complete
list of available priors.

\phantomsection\label{\detokenize{keywords/index:field}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{FIELD}

The FIELD keyword is used to parametrize quantities which have extent over the
full grid. Both dynamic properties like pressure, and static properties like
porosity, are implemented in terms of FIELD objects. When adding fields in the
config file the syntax is a bit different for dynamic fields (typically
solution data from ECLIPSE) and parameter fields like permeability and
porosity.

\sphinxstylestrong{Dynamic fields}

To add a dynamic field the entry in the configuration file looks like this:
\begin{description}
\item[{::}] \leavevmode
FIELD   \textless{}ID\textgreater{}   DYNAMIC  MIN:X  MAX:Y

\end{description}

In this case ID is not an arbitrary string; it must coincide with the keyword
name found in the ECLIPSE restart file, e.g. PRESSURE. Optionally, you can add
a minimum and/or a maximum value with MIN:X and MAX:Y.

\sphinxstyleemphasis{Example A:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Adding} \PYG{n}{pressure} \PYG{n}{field} \PYG{p}{(}\PYG{n}{unbounded}\PYG{p}{)}
\PYG{n}{FIELD} \PYG{n}{PRESSURE} \PYG{n}{DYNAMIC}
\end{sphinxVerbatim}

\sphinxstyleemphasis{Example B:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Adding} \PYG{n}{a} \PYG{n}{bounded} \PYG{n}{water} \PYG{n}{saturation} \PYG{n}{field}
\PYG{n}{FIELD} \PYG{n}{SWAT} \PYG{n}{DYNAMIC} \PYG{n}{MIN}\PYG{p}{:}\PYG{l+m+mf}{0.2} \PYG{n}{MAX}\PYG{p}{:}\PYG{l+m+mf}{0.95}
\end{sphinxVerbatim}

\sphinxstylestrong{Parameter fields}

A parameter field (e.g. porosity or permeability) is defined as follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{FIELD}  \PYG{n}{ID} \PYG{n}{PARAMETER}   \PYG{o}{\PYGZlt{}}\PYG{n}{ECLIPSE\PYGZus{}FILE}\PYG{o}{\PYGZgt{}}  \PYG{n}{INIT\PYGZus{}FILES}\PYG{p}{:}\PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{o}{\PYGZpc{}}\PYG{n}{d}  \PYG{n}{MIN}\PYG{p}{:}\PYG{n}{X} \PYG{n}{MAX}\PYG{p}{:}\PYG{n}{Y} \PYG{n}{OUTPUT\PYGZus{}TRANSFORM}\PYG{p}{:}\PYG{n}{FUNC} \PYG{n}{INIT\PYGZus{}TRANSFORM}\PYG{p}{:}\PYG{n}{FUNC}
\end{sphinxVerbatim}

Here ID is again an arbitrary string, ECLIPSE\_FILE is the name of the file the
enkf will export this field to when running simulations. Note that there
should be an IMPORT statement in the ECLIPSE data file corresponding to the
name given with ECLIPSE\_FILE. INIT\_FILES is a filename (with an embedded \%d)
to load the initial field from. Can be RMS ROFF format, ECLIPSE restart format
or ECLIPSE GRDECL format.

The input arguments MIN, MAX, INIT\_TRANSFORM and OUTPUT\_TRANSFORM are all
optional. MIN and MAX are as for dynamic fields.

For Assisted history matching, the variables in ERT should be normally
distributed internally - the purpose of the transformations is to enable
working with normally distributed variables internally in ERT. Thus, the
optional arguments INIT\_TRANSFORM:FUNC and OUTPUT\_TRANSFORM:FUNC are used to
transform the user input of parameter distribution. INIT\_TRANSFORM:FUNC is a
function which will be applied when they are loaded to ERT.
OUTPUT\_TRANSFORM:FUNC is a function which will be applied to the field when it
is exported from ERT, and FUNC is the name of a transformation function to be
applied. The avaialble functions are listed below:

“POW10”       : This function will raise x to the power of 10: y = 10\textasciicircum{}x.
“TRUNC\_POW10” : This function will raise x to the power of 10 - and truncate lower values at 0.001.
“LOG”         : This function will take the NATURAL logarithm of x: y = ln(x).
“LN”          : This function will take the NATURAL logarithm of x: y = ln(x).
“LOG10”       : This function will take the log10 logarithm of x: y = log10(x).
“EXP”         : This function will calculate y = exp(x).
“LN0”         : This function will calculate y = ln(x + 0.000001
“EXP0”        : This function will calculate y = exp(x) - 0.000001

For example, the most common scenario is that underlying log-normal
distributed permeability in RMS are transformed to normally distributted in
ERT, then you do:

INIT\_TRANSFORM:LOG To ensure that the variables which were initially
log-normal distributed are transformed to normal distribution when they are
loaded into ert.

OUTPUT\_TRANSFORM:EXP To ensure that the variables are reexponentiated to be
log-normal distributed before going out to Eclipse.

If users specify the wrong function name (e.g INIT\_TRANSFORM:I\_DONT\_KNOW), ERT
will stop and print all the valid function names.

Regarding format of ECLIPSE\_FILE: The default format for the parameter fields
is binary format of the same type as used in the ECLIPSE restart files. This
requires that the ECLIPSE datafile contains an IMPORT statement. The advantage
with using a binary format is that the files are smaller, and reading/writing
is faster than for plain text files. If you give the ECLIPSE\_FILE with the
extension .grdecl (arbitrary case), enkf will produce ordinary .grdecl files,
which are loaded with an INCLUDE statement. This is probably what most users
are used to beforehand - but we recomend the IMPORT form.

\sphinxstylestrong{General fields}

In addition to dynamic and parameter field there is also a general field,
where you have fine grained control over input/output. Use of the general
field type is only relevant for advanced features. The arguments for the
general field type are as follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{FIELD}   \PYG{n}{ID}  \PYG{n}{GENERAL}    \PYG{n}{FILE\PYGZus{}GENERATED\PYGZus{}BY\PYGZus{}ENKF}  \PYG{n}{FILE\PYGZus{}LOADED\PYGZus{}BY\PYGZus{}ENKF}    \PYG{o}{\PYGZlt{}}\PYG{n}{OPTIONS}\PYG{o}{\PYGZgt{}}
\end{sphinxVerbatim}

The OPTIONS argument is the same as for the parameter field.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:gen-data}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{GEN\_DATA}
\begin{quote}
\begin{quote}

The GEN\_DATA keyword is used when estimating data types which enkf does not
know anything about. GEN\_DATA is very similar to GEN\_PARAM, but GEN\_DATA is
used for data which are updated/created by the forward model like e.g. seismic
data. In the main configuration file the input for a GEN\_DATA instance is as
follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GEN\PYGZus{}DATA}  \PYG{n}{ID} \PYG{n}{RESULT\PYGZus{}FILE}\PYG{p}{:}\PYG{n}{yyy} \PYG{n}{INPUT\PYGZus{}FORMAT}\PYG{p}{:}\PYG{n}{xx}  \PYG{n}{REPORT\PYGZus{}STEPS}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{20}  \PYG{n}{ECL\PYGZus{}FILE}\PYG{p}{:}\PYG{n}{xxx}  \PYG{n}{OUTPUT\PYGZus{}FORMAT}\PYG{p}{:}\PYG{n}{xx}  \PYG{n}{INIT\PYGZus{}FILES}\PYG{p}{:}\PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{files}\PYG{o}{\PYGZpc{}}\PYG{n}{d} \PYG{n}{TEMPLATE}\PYG{p}{:}\PYG{o}{/}\PYG{n}{template\PYGZus{}file} \PYG{n}{TEMPLATE\PYGZus{}KEY}\PYG{p}{:}\PYG{n}{magic\PYGZus{}string}
\end{sphinxVerbatim}

The GEN\_DATA keyword has many options; in many cases you can leave many of
them off. We therefor list the required and the optional options separately:

\sphinxstylestrong{Required GEN\_DATA options}
\begin{itemize}
\item {} 
RESULT\_FILE - This if the name the file generated by the forward model and

\end{itemize}
\end{quote}

read by ERT. This filename \_must\_ have a \%d as part of the name, that \%d
will be replaced by report step when loading.
\begin{itemize}
\item {} 
INPUT\_FORMAT - The format of the file written by the forward model (i.e.

\end{itemize}

RESULT\_FILE) and read by ERT, valid values are ASCII, BINARY\_DOUBLE and
BINARY\_FLOAT.
\begin{itemize}
\item {} 
REPORT\_STEPS A list of the report step(s) where you expect the forward model

\end{itemize}

to create a result file. I.e. if the forward model should create a result
file for report steps 50 and 100 this setting should be:
REPORT\_STEPS:50,100. If you have observations of this GEN\_DATA data the
RESTART setting of the corresponding GENERAL\_OBSERVATION must match one of
the values given by REPORT\_STEPS.
\begin{quote}

\sphinxstylestrong{Optional GEN\_DATA options}
\begin{itemize}
\item {} 
ECL\_FILE - This is the name of file written by enkf to be read by the

\end{itemize}
\end{quote}
\begin{description}
\item[{forward model.}] \leavevmode\begin{itemize}
\item {} 
OUTPUT\_FORMAT - The format of the files written by enkf and read by the

\end{itemize}

\end{description}

forward model, valid values are ASCII, BINARY\_DOUBLE, BINARY\_FLOAT and
ASCII\_TEMPLATE. If you use ASCII\_TEMPLATE you must also supply values for
TEMPLATE and TEMPLATE\_KEY.
\begin{quote}
\begin{itemize}
\item {} 
INIT\_FILES - Format string with ‘\%d’ of files to load the initial data from.

\end{itemize}

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GEN\PYGZus{}DATA} \PYG{l+m+mi}{4}\PYG{n}{DWOC}  \PYG{n}{INPUT\PYGZus{}FORMAT}\PYG{p}{:}\PYG{n}{ASCII}   \PYG{n}{RESULT\PYGZus{}FILE}\PYG{p}{:}\PYG{n}{SimulatedWOC}\PYG{o}{\PYGZpc{}}\PYG{n}{d}\PYG{o}{.}\PYG{n}{txt}   \PYG{n}{REPORT\PYGZus{}STEPS}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

Here we introduce a GEN\_DATA instance with name 4DWOC. When the forward model
has run it should create two files with name SimulatedWOC10.txt and
SimulatedWOC100.txt. The result files are in ASCII format, ERT will look for
these files and load the content. The files should be pure numbers - without
any header.

{\color{red}\bfseries{}**}Observe that the GEN\_DATA RESULT\_FILE setting must have a \%d format
\end{quote}
\end{quote}

specifier, that will be replaced with the report step..**
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:custom-kw}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{CUSTOM\_KW}

The keyword CUSTOM\_KW enables custom data key:value pairs
to be stored in ERT storage.  Custom KW has many
similarities to Gen KW and Gen Data but is fully defined by
the user and contain only key\_value pairs.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{CUSTOM\PYGZus{}KW} \PYG{n}{GROUP\PYGZus{}NAME} \PYG{o}{\PYGZlt{}}\PYG{n}{input\PYGZus{}file}\PYG{o}{\PYGZgt{}}

\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{GROUP\PYGZus{}NAME}
\PYG{n}{This} \PYG{o+ow}{is} \PYG{n}{similar} \PYG{n}{to} \PYG{n}{Gen} \PYG{n}{KW} \PYG{n}{where} \PYG{n}{every} \PYG{n}{keyword} \PYG{o+ow}{is} \PYG{n}{prefixed} \PYG{k}{with} \PYG{n}{the} \PYG{n}{GROUP\PYGZus{}NAME} \PYG{n}{like} \PYG{n}{this}\PYG{p}{:} \PYG{n}{GROUP\PYGZus{}NAME}\PYG{p}{:}\PYG{n}{KEYWORD}

\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{input\PYGZus{}file}
\PYG{n}{This} \PYG{o+ow}{is} \PYG{n}{the} \PYG{n+nb}{input} \PYG{n}{file} \PYG{n}{expected} \PYG{n}{to} \PYG{n}{be} \PYG{n}{generated} \PYG{n}{by} \PYG{n}{a} \PYG{n}{forward} \PYG{n}{model}\PYG{o}{.}

\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{Example}
\PYG{n}{CUSTOM\PYGZus{}KW} \PYG{n}{COMPOSITION} \PYG{n}{composition}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

With this setup ERT will expect the file composition.txt to be present in the runpath.
This file may look like this

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{oil} \PYG{l+m+mf}{0.5}
\PYG{n}{water} \PYG{l+m+mf}{0.2}
\PYG{n}{gas} \PYG{l+m+mf}{0.2}
\PYG{n}{unknown} \PYG{l+m+mf}{0.1}
\PYG{n}{state} \PYG{n}{good}
\end{sphinxVerbatim}

Every key-value pair must be a string followed by a space and a value.
The value can either be a number or a string (all numbers are interpreted as floats).

After a successful run, ERT will store the COMPOSITION
Custom KW in its filesystem and will be available for every
realization.  An export will present the values produced as:
\begin{itemize}
\item {} 
COMPOSITION:oil

\item {} 
COMPOSITION:water

\item {} 
COMPOSITION:gas

\item {} 
COMPOSITION:unknown

\item {} 
COMPOSITION:state

\end{itemize}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:gen-kw}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{GEN\_KW}

The GEN\_KW (abbreviation of general keyword) parameter is based on a template
file and substitution. In the main config file a GEN\_KW instance is defined as
follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GEN\PYGZus{}KW}  \PYG{n}{ID}  \PYG{n}{my\PYGZus{}template}\PYG{o}{.}\PYG{n}{txt}  \PYG{n}{my\PYGZus{}eclipse\PYGZus{}include}\PYG{o}{.}\PYG{n}{txt}  \PYG{n}{my\PYGZus{}priors}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

Here ID is an (arbitrary) unique string, my\_template.txt is the name of a
template file, my\_eclipse\_include.txt is the name of the file which is made
for each member based on my\_template.txt and my\_priors.txt is a file
containing a list of parametrized keywords and a prior distribution for each.
Note that you must manually edit the ECLIPSE data file so that
my\_eclipse\_include.txt is included.

Let us consider an example where the GEN\_KW parameter type is used to estimate
pore volume multipliers. We would then declare a GEN\_KW instance in the main
enkf configuration file:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GEN\PYGZus{}KW} \PYG{n}{PAR\PYGZus{}MULTPV} \PYG{n}{multpv\PYGZus{}template}\PYG{o}{.}\PYG{n}{txt} \PYG{n}{multpv}\PYG{o}{.}\PYG{n}{txt} \PYG{n}{multpv\PYGZus{}priors}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

In the GRID or EDIT section of the ECLIPSE data file, we would insert the
following include statement:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{INCLUDE}
 \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{multpv.txt}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{/}
\end{sphinxVerbatim}

The template file multpv\_template.txt would contain some parametrized ECLIPSE
statements:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{BOX}
 \PYG{l+m+mi}{1} \PYG{l+m+mi}{10} \PYG{l+m+mi}{1} \PYG{l+m+mi}{30} \PYG{l+m+mi}{13} \PYG{l+m+mi}{13} \PYG{o}{/}
\PYG{n}{MULTPV}
 \PYG{l+m+mi}{300}\PYG{o}{*}\PYG{o}{\PYGZlt{}}\PYG{n}{MULTPV\PYGZus{}BOX1}\PYG{o}{\PYGZgt{}} \PYG{o}{/}
\PYG{n}{ENDBOX}

\PYG{n}{BOX}
 \PYG{l+m+mi}{1} \PYG{l+m+mi}{10} \PYG{l+m+mi}{1} \PYG{l+m+mi}{30} \PYG{l+m+mi}{14} \PYG{l+m+mi}{14} \PYG{o}{/}
\PYG{n}{MULTPV}
 \PYG{l+m+mi}{300}\PYG{o}{*}\PYG{o}{\PYGZlt{}}\PYG{n}{MULTPV\PYGZus{}BOX2}\PYG{o}{\PYGZgt{}} \PYG{o}{/}
\PYG{n}{ENDBOX}
\end{sphinxVerbatim}

Here, \textless{}MULTPV\_BOX1\textgreater{} and \textless{}MULTPV\_BOX2\textgreater{} will act as magic strings. Note that the
‘\textless{}’ ‘\textgreater{}’ must be present around the magic strings. In this case, the parameter
configuration file multpv\_priors.txt could look like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{MULTPV\PYGZus{}BOX2} \PYG{n}{UNIFORM} \PYG{l+m+mf}{0.98} \PYG{l+m+mf}{1.03}
\PYG{n}{MULTPV\PYGZus{}BOX1} \PYG{n}{UNIFORM} \PYG{l+m+mf}{0.85} \PYG{l+m+mf}{1.00}
\end{sphinxVerbatim}

In general, the first keyword on each line in the parameter configuration file
defines a key, which when found in the template file enclosed in ‘\textless{}’ and ‘\textgreater{}’,
is replaced with a value. The rest of the line defines a prior distribution
for the key. See Prior distributions available in enkf for a list of available
prior distributions.

\sphinxstylestrong{Example: Using GEN\_KW to estimate fault transmissibility multipliers}

Previously enkf supported a datatype MULTFLT for estimating fault
transmissibility multipliers. This has now been depreceated, as the
functionality can be easily achieved with the help of GEN\_KW. In th enkf
config file:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GEN\PYGZus{}KW}  \PYG{n}{MY}\PYG{o}{\PYGZhy{}}\PYG{n}{FAULTS}   \PYG{n}{MULTFLT}\PYG{o}{.}\PYG{n}{tmpl}   \PYG{n}{MULTFLT}\PYG{o}{.}\PYG{n}{INC}   \PYG{n}{MULTFLT}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

Here MY-FAULTS is the (arbitrary) key assigned to the fault multiplers,
MULTFLT.tmpl is the template file, which can look like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{MULTFLT}
 \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FAULT1}\PYG{l+s+s1}{\PYGZsq{}}   \PYG{o}{\PYGZlt{}}\PYG{n}{FAULT1}\PYG{o}{\PYGZgt{}}  \PYG{o}{/}
 \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FAULT2}\PYG{l+s+s1}{\PYGZsq{}}   \PYG{o}{\PYGZlt{}}\PYG{n}{FAULT2}\PYG{o}{\PYGZgt{}}  \PYG{o}{/}
\PYG{o}{/}
\end{sphinxVerbatim}

and finally the initial distribution of the parameters FAULT1 and FAULT2 are
defined in the file MULTFLT.txt:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{FAULT1}   \PYG{n}{LOGUNIF}   \PYG{l+m+mf}{0.00001}   \PYG{l+m+mf}{0.1}
\PYG{n}{FAULT2}   \PYG{n}{UNIFORM}   \PYG{l+m+mf}{0.00}      \PYG{l+m+mf}{1.0}
\end{sphinxVerbatim}

The various prior distributions available for the \sphinxcode{\sphinxupquote{GEN\_KW}}
keyword are here \DUrole{xref,std,std-ref}{prior distributions available in ERT}

Loading GEN\_KW values from an external file

The default use of the GEN\_KW keyword is to let the ERT application sample
random values for the elements in the GEN\_KW instance, but it is also possible
to tell ERT to load a precreated set of data files, this can for instance be
used as a component in a experimental design based workflow. When using
external files to initialize the GEN\_KW instances you supply an extra keyword
\sphinxcode{\sphinxupquote{INIT\_FILE:/path/to/priors/files\%d}} which tells where the prior files are:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GEN\PYGZus{}KW}  \PYG{n}{MY}\PYG{o}{\PYGZhy{}}\PYG{n}{FAULTS}   \PYG{n}{MULTFLT}\PYG{o}{.}\PYG{n}{tmpl}   \PYG{n}{MULTFLT}\PYG{o}{.}\PYG{n}{INC}   \PYG{n}{MULTFLT}\PYG{o}{.}\PYG{n}{txt}    \PYG{n}{INIT\PYGZus{}FILES}\PYG{p}{:}\PYG{n}{priors}\PYG{o}{/}\PYG{n}{multflt}\PYG{o}{/}\PYG{n}{faults}\PYG{o}{\PYGZpc{}}\PYG{n}{d}
\end{sphinxVerbatim}

In the example above you must prepare files priors/multflt/faults0,
priors/multflt/faults1, … priors/multflt/faultsn which ert will load when
you initialize the case. The format of the GEN\_KW input files can be of two
varieties:
\begin{enumerate}
\item {} 
The files can be plain ASCII text files with a list of numbers:

\end{enumerate}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mf}{1.25}
\PYG{l+m+mf}{2.67}
\end{sphinxVerbatim}

The numbers will be assigned to parameters in the order found in the
MULTFLT.txt file.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Alternatively values and keywords can be interleaved as in:

\end{enumerate}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{FAULT1} \PYG{l+m+mf}{1.25}
\PYG{n}{FAULT2} \PYG{l+m+mf}{2.56}
\end{sphinxVerbatim}

in this case the ordering can differ in the init files and the parameter file.

The heritage of the ERT program is based on the EnKF algorithm, and the EnKF
algorithm evolves around Gaussian variables - internally the GEN\_KW variables
are assumed to be samples from the N(0,1) distribution, and the distributions
specified in the parameters file are based on transformations starting with a
N(0,1) distributed variable. The slightly awkward consequence of this is that
to let your sampled values pass through ERT unmodified you must configure the
distribution NORMAL 0 1 in the parameter file; alternatively if you do not
intend to update the GEN\_KW variable you can use the distribution RAW.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:gen-param}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{GEN\_PARAM}
\begin{quote}
\begin{quote}

The GEN\_PARAM parameter type is used to estimate parameters which do not
really fit into any of the other categories. As an example, consider the
following situation:

Some external Software (e.g. Cohiba) makes a large vector of random numbers
which will serve as input to the forward model. (It is no requirement that the
parameter set is large, but if it only consists of a few parameters the GEN\_KW
type will be easier to use.) We want to update this parameter with enkf. In
the main configuration file the input for a GEN\_PARAM instance is as follows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{GEN\PYGZus{}PARAM}  \PYG{n}{ID}  \PYG{n}{ECLIPSE\PYGZus{}FILE}  \PYG{n}{INPUT\PYGZus{}FORMAT}\PYG{p}{:}\PYG{n}{xx}  \PYG{n}{OUTPUT\PYGZus{}FORMAT}\PYG{p}{:}\PYG{n}{xx}  \PYG{n}{INIT\PYGZus{}FILES}\PYG{p}{:}\PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{init}\PYG{o}{/}\PYG{n}{files}\PYG{o}{\PYGZpc{}}\PYG{n}{d} \PYG{p}{(}\PYG{n}{TEMPLATE}\PYG{p}{:}\PYG{o}{/}\PYG{n}{template\PYGZus{}file} \PYG{n}{KEY}\PYG{p}{:}\PYG{n}{magic\PYGZus{}string}\PYG{p}{)}
\end{sphinxVerbatim}

here ID is the usual unique string identifying this instance and ECLIPSE\_FILE
is the name of the file which is written into the run directories. The three
arguments GEN\_PARAM, ID and ECLIPSE\_FILE must be the three first arguments. In
addition you must have three additional arguments, INPUT\_FORMAT, OUTPUT\_FORMAT
and INIT\_FILES. INPUT\_FORMAT is the format of the files enkf should load to
initialize, and OUTPUT\_FORMAT is the format of the files enkf writes for the
forward model. The valid values are:
\begin{itemize}
\item {} 
ASCII - This is just text file with formatted numbers.

\item {} 
ASCII\_TEMPLATE - An plain text file with formatted numbers, and an arbitrary

\end{itemize}
\end{quote}
\begin{description}
\item[{header/footer.}] \leavevmode\begin{quote}
\begin{itemize}
\item {} 
BINARY\_FLOAT - A vector of binary float numbers.

\item {} 
BINARY\_DOUBLE - A vector of binary double numbers.

\end{itemize}

Regarding the different formats - observe the following:
\begin{enumerate}
\item {} 
Except the format ASCII\_TEMPLATE the files contain no header information.

\item {} 
The format ASCII\_TEMPLATE can only be used as output format.

\item {} 
If you use the output format ASCII\_TEMPLATE you must also supply a

\end{enumerate}
\end{quote}
\begin{description}
\item[{TEMPLATE:X and KEY:Y option. See documentation of this below.}] \leavevmode\begin{enumerate}
\item {} 
For the binary formats files generated by Fortran can not be used - can

\end{enumerate}

\end{description}

easily be supported on request.
\begin{quote}

\sphinxstylestrong{Regarding templates:} If you use OUTPUT\_FORMAT:ASCII\_TEMPLATE you must also
\end{quote}

\end{description}
\end{quote}

supply the arguments TEMPLATE:/template/file and KEY:MaGiCKEY. The template
file is an arbitrary existing text file, and KEY is a magic string found in
this file. When enkf is running the magic string is replaced with parameter
data when the ECLIPSE\_FILE is written to the directory where the simulation
is run from. Consider for example the follwing configuration:
\begin{quote}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{TEMPLATE}\PYG{p}{:}\PYG{o}{/}\PYG{n}{some}\PYG{o}{/}\PYG{n}{file}   \PYG{n}{KEY}\PYG{p}{:}\PYG{n}{Magic123}
\end{sphinxVerbatim}

The template file can look like this (only the Magic123 is special):

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Header} \PYG{n}{line1}
\PYG{n}{Header} \PYG{n}{line2}
\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}
\PYG{n}{Magic123}
\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}
\PYG{n}{Footer} \PYG{n}{line1}
\PYG{n}{Footer} \PYG{n}{line2}
\end{sphinxVerbatim}

When enkf is running the string Magic123 is replaced with parameter values,
and the resulting file will look like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Header} \PYG{n}{line1}
\PYG{n}{Header} \PYG{n}{line2}
\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}
\PYG{l+m+mf}{1.6723}
\PYG{l+m+mf}{5.9731}
\PYG{l+m+mf}{4.8881}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}\PYG{o}{==}
\PYG{n}{Footer} \PYG{n}{line1}
\PYG{n}{Footer} \PYG{n}{line2}
\end{sphinxVerbatim}
\end{quote}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:surface}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{SURFACE}
\begin{quote}

The SURFACE keyword can be used to work with surface from RMS in the irap
format. The surface keyword is configured like this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{SURFACE} \PYG{n}{TOP}   \PYG{n}{OUTPUT\PYGZus{}FILE}\PYG{p}{:}\PYG{n}{surf}\PYG{o}{.}\PYG{n}{irap}   \PYG{n}{INIT\PYGZus{}FILES}\PYG{p}{:}\PYG{n}{Surfaces}\PYG{o}{/}\PYG{n}{surf}\PYG{o}{\PYGZpc{}}\PYG{n}{d}\PYG{o}{.}\PYG{n}{irap}   \PYG{n}{BASE\PYGZus{}SURFACE}\PYG{p}{:}\PYG{n}{Surfaces}\PYG{o}{/}\PYG{n}{surf0}\PYG{o}{.}\PYG{n}{irap}
\end{sphinxVerbatim}

The first argument, TOP in the example above, is the identifier you want to
use for this surface in ert. The OUTPUT\_FILE key is the name of surface file
which ERT will generate for you, INIT\_FILES points to a list of files which
are used to initialize, and BASE\_SURFACE must point to one existing surface
file. When loading the surfaces ERT will check that all the headers are
compatible. An example of a surface IRAP file is:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{996}   \PYG{l+m+mi}{511}     \PYG{l+m+mf}{50.000000}     \PYG{l+m+mf}{50.000000}
\PYG{l+m+mf}{444229.9688}   \PYG{l+m+mf}{457179.9688}  \PYG{l+m+mf}{6809537.0000}  \PYG{l+m+mf}{6835037.0000}
\PYG{l+m+mi}{260}      \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{30.0000}   \PYG{l+m+mf}{444229.9688}  \PYG{l+m+mf}{6809537.0000}
\PYG{l+m+mi}{0}     \PYG{l+m+mi}{0}     \PYG{l+m+mi}{0}     \PYG{l+m+mi}{0}     \PYG{l+m+mi}{0}     \PYG{l+m+mi}{0}     \PYG{l+m+mi}{0}
\PYG{l+m+mf}{2735.7461}    \PYG{l+m+mf}{2734.8909}    \PYG{l+m+mf}{2736.9705}    \PYG{l+m+mf}{2737.4048}    \PYG{l+m+mf}{2736.2539}    \PYG{l+m+mf}{2737.0122}
\PYG{l+m+mf}{2740.2644}    \PYG{l+m+mf}{2738.4014}    \PYG{l+m+mf}{2735.3770}    \PYG{l+m+mf}{2735.7327}    \PYG{l+m+mf}{2733.4944}    \PYG{l+m+mf}{2731.6448}
\PYG{l+m+mf}{2731.5454}    \PYG{l+m+mf}{2731.4810}    \PYG{l+m+mf}{2730.4644}    \PYG{l+m+mf}{2730.5591}    \PYG{l+m+mf}{2729.8997}    \PYG{l+m+mf}{2726.2217}
\PYG{l+m+mf}{2721.0996}    \PYG{l+m+mf}{2716.5913}    \PYG{l+m+mf}{2711.4338}    \PYG{l+m+mf}{2707.7791}    \PYG{l+m+mf}{2705.4504}    \PYG{l+m+mf}{2701.9187}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

The surface data will typically be fed into other programs like Cohiba or RMS.
The data can be updated using e.g. the Smoother.

\sphinxstylestrong{Initializing from the FORWARD MODEL}

All the parameter types like FIELD,GEN\_KW,GEN\_PARAM and SURFACE can be
initialized from the forward model. To achieve this you just add the setting
FORWARD\_INIT:True to the configuration. When using forward init the
initialization will work like this:
\begin{enumerate}
\item {} 
The explicit initialization from the case menu, or when you start a

\end{enumerate}
\end{quote}
\begin{description}
\item[{simulation, will be ignored.}] \leavevmode\begin{enumerate}
\item {} 
When the FORWARD\_MODEL is complete ERT will try to initialize the node

\end{enumerate}

\end{description}

based on files created by the forward model. If the init fails the job as a
whole will fail.
\begin{enumerate}
\item {} 
If a node has been initialized, it will not be initialized again if you run

\end{enumerate}

again. {[}Should be possible to force this ….{]}
\begin{quote}

When using FORWARD\_INIT:True ERT will consider the INIT\_FILES setting to find
which file to initialize from. If the INIT\_FILES setting contains a relative
filename, it will be interpreted relativt to the runpath directory. In the
example below we assume that RMS has created a file petro.grdecl which
contains both the PERMX and the PORO fields in grdecl format; we wish to
initialize PERMX and PORO nodes from these files:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{FIELD}   \PYG{n}{PORO}  \PYG{n}{PARAMETER}    \PYG{n}{poro}\PYG{o}{.}\PYG{n}{grdecl}     \PYG{n}{INIT\PYGZus{}FILES}\PYG{p}{:}\PYG{n}{petro}\PYG{o}{.}\PYG{n}{grdecl}  \PYG{n}{FORWARD\PYGZus{}INIT}\PYG{p}{:}\PYG{k+kc}{True}
\PYG{n}{FIELD}   \PYG{n}{PERMX} \PYG{n}{PARAMETER}    \PYG{n}{permx}\PYG{o}{.}\PYG{n}{grdecl}    \PYG{n}{INIT\PYGZus{}FILES}\PYG{p}{:}\PYG{n}{petro}\PYG{o}{.}\PYG{n}{grdecl}  \PYG{n}{FORWARD\PYGZus{}INIT}\PYG{p}{:}\PYG{k+kc}{True}
\end{sphinxVerbatim}

Observe that forward model has created the file petro.grdecl and the nodes
PORO and PERMX create the ECLIPSE input files poro.grdecl and permx.grdecl, to
ensure that ECLIPSE finds the input files poro.grdecl and permx.grdecl the
forward model should contain a job which will copy/convert petro.grdecl -\textgreater{}
(poro.grdecl,permx.grdecl), this job should not overwrite existing versions of
permx.grdecl and poro.grdecl. This extra hoops is not strictly needed in all
cases, but strongly recommended to ensure that you have control over which
data is used, and that everything is consistent in the case where the forward
model is run again.
\end{quote}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:summary}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{SUMMARY}

The SUMMARY keyword is used to add variables from the ECLIPSE summary file to
the parametrization. The keyword expects a string, which should have the
format VAR:WGRNAME. Here, VAR should be a quantity, such as WOPR, WGOR, RPR or
GWCT. Moreover, WGRNAME should refer to a well, group or region. If it is a
field property, such as FOPT, WGRNAME need not be set to FIELD.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Using} \PYG{n}{the} \PYG{n}{SUMMARY} \PYG{n}{keyword} \PYG{n}{to} \PYG{n}{add} \PYG{n}{diagnostic} \PYG{n}{variables}
\PYG{n}{SUMMARY} \PYG{n}{WOPR}\PYG{p}{:}\PYG{n}{MY\PYGZus{}WELL}
\PYG{n}{SUMMARY} \PYG{n}{RPR}\PYG{p}{:}\PYG{l+m+mi}{8}
\PYG{n}{SUMMARY} \PYG{n}{F}\PYG{o}{*}          \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Use} \PYG{n}{of} \PYG{n}{wildcards} \PYG{n}{requires} \PYG{n}{that} \PYG{n}{you} \PYG{n}{have} \PYG{n}{entered} \PYG{n}{a} \PYG{n}{REFCASE}\PYG{o}{.}
\end{sphinxVerbatim}

The SUMMARY keyword has limited support for ‘*’ wildcards, if your key
contains one or more ‘*’ characters all matching variables from the refcase
are selected. Observe that if your summary key contains wildcards you must
supply a refcase with the REFCASE key - otherwise it will fail hard.

\sphinxstylestrong{Note:} Properties added using the SUMMARY keyword are only diagnostic. I.e., they have no effect on the sensitivity analysis or history match.
\end{sphinxShadowBox}


\section{Keywords controlling the ES algorithm}
\label{\detokenize{keywords/index:keywords-controlling-the-es-algorithm}}\phantomsection\label{\detokenize{keywords/index:id7}}\phantomsection\label{\detokenize{keywords/index:enkf-alpha}}
See the sub keyword \sphinxcode{\sphinxupquote{OVERLAP\_LIMIT}} under the :code:{\color{red}\bfseries{}{}`}UPDATE\_SETTINGS{}`keyword.

\phantomsection\label{\detokenize{keywords/index:enkf-bootstrap}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_BOOTSTRAP}

Boolean specifying if we want to resample the Kalman gain matrix in the update
step. The purpose is to avoid that the ensemble covariance collapses. When
this keyword is true each ensemble member will be updated based on a Kalman
gain matrix estimated from a resampling with replacement of the full ensemble.

In theory and in practice this has worked well when one uses a small number of
ensemble members.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-cv-folds}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_CV\_FOLDS}

Integer specifying how many folds we should use in the Cross-Validation (CV)
scheme. Possible choices are the integers between 2 and the ensemble size
(2-fold CV and leave-one-out CV respectively). However, a robust choice for
the number of CV-folds is 5 or 10 (depending on the ensemble size).

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Setting} \PYG{n}{the} \PYG{n}{number} \PYG{n}{of} \PYG{n}{CV} \PYG{n}{folds} \PYG{n}{equal} \PYG{n}{to} \PYG{l+m+mi}{5}
\PYG{n}{ENKF\PYGZus{}CV\PYGZus{}FOLDS} \PYG{l+m+mi}{5}
\end{sphinxVerbatim}

Requires that the ENKF\_LOCAL\_CV keyword is set to TRUE
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-force-ncomp}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_FORCE\_NCOMP}

Bool specifying if we want to force the subspace dimension we want to use in
the EnKF updating scheme (SVD-based) to a specific integer. This is an
alternative to selecting the dimension using ENKF\_TRUNCATION or ENKF\_LOCAL\_CV.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Setting} \PYG{n}{the} \PYG{n}{the} \PYG{n}{subspace} \PYG{n}{dimension} \PYG{n}{to} \PYG{l+m+mi}{2}
\PYG{n}{ENKF\PYGZus{}FORCE\PYGZus{}NCOMP}     \PYG{n}{TRUE}
\PYG{n}{ENKF\PYGZus{}NCOMP}              \PYG{l+m+mi}{2}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-local-cv}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_LOCAL\_CV}

Boolean specifying if we want to select the subspace dimension in the
SVD-based EnKF algorithm using Cross-Validation (CV) {[}1{]}. This is a more
robust alternative to selecting the subspace dimension based on the estimated
singular values (See ENKF\_TRUNCATION), because the predictive power of the
estimated Kalman gain matrix is taken into account.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Select} \PYG{n}{the} \PYG{n}{subspace} \PYG{n}{dimension} \PYG{n}{using} \PYG{n}{Cross}\PYG{o}{\PYGZhy{}}\PYG{n}{Validation}
\PYG{n}{ENKF\PYGZus{}LOCAL\PYGZus{}CV} \PYG{n}{TRUE}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-pen-press}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_PEN\_PRESS}

Boolean specifying if we want to select the subspace dimension in the
SVD-based EnKF algorithm using Cross-Validation (CV), and a penalised version
of the predictive error sum of squares (PRESS) statistic {[}2{]}. This is
recommended when overfitting is a severe problem (and when the number of
ensemble members is small)

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Select} \PYG{n}{the} \PYG{n}{subspace} \PYG{n}{dimension} \PYG{n}{using} \PYG{n}{Cross}\PYG{o}{\PYGZhy{}}\PYG{n}{Validation}
\PYG{n}{ENKF\PYGZus{}LOCAL\PYGZus{}CV} \PYG{n}{TRUE}

\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Using} \PYG{n}{penalised} \PYG{n}{PRESS} \PYG{n}{statistic}
\PYG{n}{ENKF\PYGZus{}PEN\PYGZus{}PRESS} \PYG{n}{TRUE}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-mode}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_MODE}

The ENKF\_MODE keyword is used to select which EnKF algorithm to use. Use the
value STANDARD for the original EnKF algorithm, or SQRT for the so-called
square root scheme. The default value for ENKF\_MODE is STANDARD.

\sphinxstyleemphasis{Example A:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Using} \PYG{n}{the} \PYG{n}{square} \PYG{n}{root} \PYG{n}{update}
\PYG{n}{ENKF\PYGZus{}MODE} \PYG{n}{SQRT}
\end{sphinxVerbatim}

\sphinxstyleemphasis{Example B:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Using} \PYG{n}{the} \PYG{n}{standard} \PYG{n}{update}
\PYG{n}{ENKF\PYGZus{}MODE} \PYG{n}{STANDARD}
\end{sphinxVerbatim}

The ENKF\_MODE keyword is optional.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-merge-observations}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_MERGE\_OBSERVATIONS}

If you use the ENKF\_SCHED\_FILE option to jump over several dates at a time you
can choose whether you want to use all the observations in between, or just
the final. If set to TRUE, all observations will be used. If set to FALSE,
only the final observation is used. The default value for
ENKF\_MERGE\_OBSERVATIONS is FALSE.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Merge} \PYG{n}{observations}
\PYG{n}{ENKF\PYGZus{}MERGE\PYGZus{}OBSERVATIONS} \PYG{n}{TRUE}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-ncomp}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_NCOMP}

Integer specifying the subspace dimension. Requires that ENKF\_FORCE\_NCOMP is
TRUE.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-rerun}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_RERUN}

This is a boolean switch - TRUE or FALSE. Should the simulation start from
time zero after each update.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-scaling}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_SCALING}

This is a boolean switch - TRUE (Default) or FALSE. If TRUE, we scale the data
ensemble matrix to unit variance. This is generally recommended because the
SVD-based EnKF algorithm is not scale invariant.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:enkf-truncation}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ENKF\_TRUNCATION}

Truncation factor for the SVD-based EnKF algorithm (see Evensen, 2007). In
this algorithm, the forecasted data will be projected into a low dimensional
subspace before assimilation. This can substantially improve on the results
obtained with the EnKF, especially if the data ensemble matrix is highly
collinear (Saetrom and Omre, 2010). The subspace dimension, p, is selected
such that

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZbs{}\PYG{n}{frac}\PYG{p}{\PYGZob{}}\PYGZbs{}\PYG{n}{sum\PYGZus{}}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{o}{\PYGZca{}}\PYG{p}{\PYGZob{}}\PYG{n}{p}\PYG{p}{\PYGZcb{}} \PYG{n}{s\PYGZus{}i}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZob{}}\PYGZbs{}\PYG{n}{sum\PYGZus{}}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{o}{\PYGZca{}}\PYG{n}{r} \PYG{n}{s\PYGZus{}i}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}} \PYGZbs{}\PYG{n}{geq} \PYGZbs{}\PYG{n}{mathrm}\PYG{p}{\PYGZob{}}\PYG{n}{ENKF}\PYGZbs{}\PYG{n}{\PYGZus{}TRUNCATION}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
\end{sphinxVerbatim}

where si is the ith singular value of the centered data ensemble matrix and r
is the rank of this matrix. This criterion is similar to the explained
variance criterion used in Principal Component Analysis (see e.g. Mardia et
al. 1979).

The default value of ENKF\_TRUNCATION is 0.99. If ensemble collapse is a big
problem, a smaller value should be used (e.g 0.90 or smaller). However, this
does not guarantee that the problem of ensemble collapse will disappear. Note
that setting the truncation factor to 1.00, will recover the Standard-EnKF
algorithm if and only if the covariance matrix for the observation errors is
proportional to the identity matrix.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:std-scale-correlated-obs}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{STD\_SCALE\_CORRELATED\_OBS}

With this kewyord you can instruct ERT to use the simulated data to
estimate the correlations in the observations, and then inflate the
observation standard deviation as a way to estimate the real information
content in the observations. The method is based on PCA, the scaling
factor is calculated as:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZbs{}\PYG{n}{sqrt}\PYG{p}{\PYGZob{}}\PYGZbs{}\PYG{n}{frac}\PYG{p}{\PYGZob{}}\PYG{n}{N\PYGZus{}}\PYG{p}{\PYGZob{}}\PYGZbs{}\PYG{n}{sigma}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZob{}}\PYG{n}{N\PYGZus{}}\PYG{p}{\PYGZob{}}\PYGZbs{}\PYG{n}{mathrm}\PYG{p}{\PYGZob{}}\PYG{n}{obs}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

where \$N\_\{sigma\}\$ is the number of singular components, at (fixed)
truncation 0.95 and \$N\_\{mathrm\{obs\}\}\$ is the number of observations.
The STD\_SCALE\_CORRELATED\_OBS keyword will flatten all your observations,
including temporal and spatial correlations. For more fine grained
control you can use the STD\_CALE\_CORRELATED\_OBS workflow job, or even
write your own plugins.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:update-log-path}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{UPDATE\_LOG\_PATH}

A summary of the data used for updates are stored in this directory.
\end{sphinxShadowBox}

\sphinxstylestrong{References}
\begin{itemize}
\item {} 
Evensen, G. (2007). “Data Assimilation, the Ensemble Kalman Filter”, Springer.

\item {} 
Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979). “Multivariate Analysis”, Academic Press.

\item {} 
Saetrom, J. and Omre, H. (2010). “Ensemble Kalman filtering with shrinkage regression techniques”, Computational Geosciences (online first).

\end{itemize}


\section{Analysis module}
\label{\detokenize{keywords/index:analysis-module}}\phantomsection\label{\detokenize{keywords/index:id10}}
The final EnKF linear algebra is performed in an analysis module. The keywords
to load, select and modify the analysis modules are documented here.

\phantomsection\label{\detokenize{keywords/index:analysis-load}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ANALYSIS\_LOAD}

The ANALYSIS\_LOAD key is the main key to load an analysis module:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ANALYSIS\PYGZus{}LOAD} \PYG{n}{ANAME}  \PYG{n}{analysis}\PYG{o}{.}\PYG{n}{so}
\end{sphinxVerbatim}

The first argument ANAME is just an arbitrary unique name which you want to
use to refer to the module later. The second argument is the name of the
shared library file implementing the module, this can either be an absolute
path as /path/to/my/module/ana.so or a relative file name as analysis.so. The
module is loaded with dlopen() and the normal shared library search semantics
applies.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:analysis-select}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ANALYSIS\_SELECT}

This command is used to select which analysis module to actually use in the
updates:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ANALYSIS\PYGZus{}SELECT} \PYG{n}{ANAME}
\end{sphinxVerbatim}

Here ANAME is the name you have assigned to the module when loading it with
ANALYSIS\_LOAD.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:analysis-set-var}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ANALYSIS\_SET\_VAR}

The analysis modules can have internal state, like e.g. truncation cutoff
values, these values can be manipulated from the config file using the
ANALYSIS\_SET\_VAR keyword:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ANALYSIS\PYGZus{}SET\PYGZus{}VAR}  \PYG{n}{ANAME}  \PYG{n}{ENKF\PYGZus{}TRUNCATION}  \PYG{l+m+mf}{0.97}
\end{sphinxVerbatim}

To use this you must know which variables the module supports setting this
way. If you try to set an unknown variable you will get an error message on
stderr.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:analysis-copy}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ANALYSIS\_COPY}

With the ANALYSIS\_COPY keyword you can create a new instance of a module. This
can be convenient if you want to run the same algorithm with the different
settings:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ANALYSIS\PYGZus{}LOAD}   \PYG{n}{A1}  \PYG{n}{analysis}\PYG{o}{.}\PYG{n}{so}
\PYG{n}{ANALYISIS\PYGZus{}COPY}  \PYG{n}{A1}  \PYG{n}{A2}
\end{sphinxVerbatim}

We load a module analysis.so and assign the name A1; then we copy A1 -\textgreater{} A2.
The module A1 and A2 are now 100\% identical. We then set the truncation to two
different values:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ANALYSIS\PYGZus{}SET\PYGZus{}VAR} \PYG{n}{A1} \PYG{n}{ENKF\PYGZus{}TRUNCATION} \PYG{l+m+mf}{0.95}
\PYG{n}{ANALYSIS\PYGZus{}SET\PYGZus{}VAR} \PYG{n}{A2} \PYG{n}{ENKF\PYGZus{}TRUNCATION} \PYG{l+m+mf}{0.98}
\end{sphinxVerbatim}
\end{sphinxShadowBox}

\sphinxstylestrong{Developing analysis modules}

In the analysis module the update equations are formulated based on familiar
matrix expressions, and no knowledge of the innards of the ERT program are
required. Some more details of how modules work can be found here modules.txt.
In principle a module is ‘just’ a shared library following some conventions, and
if you are sufficiently savy with gcc you can build them manually, but along
with the ert installation you should have utility script ert\_module which can be
used to build a module; just write ert\_module without any arguments to get a
brief usage description.


\section{Advanced optional keywords}
\label{\detokenize{keywords/index:advanced-optional-keywords}}\phantomsection\label{\detokenize{keywords/index:id11}}
The keywords in this section, controls advanced features of the enkf
application. Insight in the internals of the enkf application and/or ECLIPSE may
be required to fully understand their effect. Moreover, many of these keywords
are defined in the site configuration, and thus optional to set for the user,
but required when installing the enkf application at a new site.

\phantomsection\label{\detokenize{keywords/index:add-fixed-length-schedule-kw}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{ADD\_FIXED\_LENGTH\_SCHEDULE\_KW}

Real low level fix for some SCHEDULE parsing problems.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:define}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{DEFINE}

With the DEFINE keyword you can define key-value pairs which will be
substituted in the rest of the configuration file. The DEFINE keyword expects
two arguments: A key and a value to replace for that key. Later instances of
the key enclosed in ‘\textless{}’ and ‘\textgreater{}’ will be substituted with the value. The value
can consist of several strings, in that case they will be joined by one single
space.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Define} \PYG{n}{ECLIPSE\PYGZus{}PATH} \PYG{o+ow}{and} \PYG{n}{ECLIPSE\PYGZus{}BASE}
\PYG{n}{DEFINE}  \PYG{n}{ECLIPSE\PYGZus{}PATH}  \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{eclipse}\PYG{o}{/}\PYG{n}{run}
\PYG{n}{DEFINE}  \PYG{n}{ECLIPSE\PYGZus{}BASE}  \PYG{n}{STATF02}
\PYG{n}{DEFINE}  \PYG{n}{KEY}           \PYG{n}{VALUE1}       \PYG{n}{VALUE2} \PYG{n}{VALUE3}            \PYG{n}{VALUE4}

\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Set} \PYG{n}{the} \PYG{n}{GRID} \PYG{o+ow}{in} \PYG{n}{terms} \PYG{n}{of} \PYG{n}{the} \PYG{n}{ECLIPSE\PYGZus{}PATH}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{o+ow}{and} \PYG{n}{ECLIPSE\PYGZus{}BASE} \PYG{n}{keys}\PYG{o}{.}
\PYG{n}{GRID}    \PYG{o}{\PYGZlt{}}\PYG{n}{ECLIPSE\PYGZus{}PATH}\PYG{o}{\PYGZgt{}}\PYG{o}{/}\PYG{o}{\PYGZlt{}}\PYG{n}{ECLIPSE\PYGZus{}BASE}\PYG{o}{\PYGZgt{}}\PYG{o}{.}\PYG{n}{EGRID}
\end{sphinxVerbatim}

Observe that when you refer to the keys later in the config file they must be
enclosed in ‘\textless{}’ and ‘\textgreater{}’. Furthermore, a key-value pair must be defined in the
config file before it can be used. The final key define above KEY, will be
replaced with VALUE1 VALUE2 VALUE3 VALUE4 - i.e. the extra spaces will be
discarded.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:time-map}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{TIME\_MAP}

Normally the mapping between report steps and true dates is inferred by
ERT indirectly by loading the ECLIPSE summary files. In cases where you
do not have any ECLIPSE summary files you can use the TIME\_MAP keyword
to specify a file with dates which are used to establish this mapping:

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Load} \PYG{n}{a} \PYG{n+nb}{list} \PYG{n}{of} \PYG{n}{dates} \PYG{k+kn}{from} \PYG{n+nn}{external} \PYG{n}{file}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{time\PYGZus{}map.txt}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{TIME\PYGZus{}MAP} \PYG{n}{time\PYGZus{}map}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

The format of the TIME\_MAP file should just be a list of dates formatted as
dd/mm/yyyy. The example file below has four dates:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{01}\PYG{o}{/}\PYG{l+m+mi}{01}\PYG{o}{/}\PYG{l+m+mi}{2000}
\PYG{l+m+mi}{01}\PYG{o}{/}\PYG{l+m+mi}{07}\PYG{o}{/}\PYG{l+m+mi}{2000}
\PYG{l+m+mi}{01}\PYG{o}{/}\PYG{l+m+mi}{01}\PYG{o}{/}\PYG{l+m+mi}{2001}
\PYG{l+m+mi}{01}\PYG{o}{/}\PYG{l+m+mi}{07}\PYG{o}{/}\PYG{l+m+mi}{2001}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:schedule-prediction-file}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{SCHEDULE\_PREDICTION\_FILE}

This is the name of a schedule prediction file. It can contain \%d to get
different files for different members. Observe that the ECLIPSE datafile
should include only one schedule file, even if you are doing predictions.
\end{sphinxShadowBox}


\section{Keywords related to running the forward model}
\label{\detokenize{keywords/index:keywords-related-to-running-the-forward-model}}\phantomsection\label{\detokenize{keywords/index:id12}}\phantomsection\label{\detokenize{keywords/index:forward-model}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{FORWARD\_MODEL}

The FORWARD\_MODEL keyword is used to define how the simulations are executed.
E.g., which version of ECLIPSE to use, which rel.perm script to run, which
rock physics model to use etc. Jobs (i.e. programs and scripts) that are to be
used in the FORWARD\_MODEL keyword must be defined using the INSTALL\_JOB
keyword. A set of default jobs are available, and by default FORWARD\_MODEL
takes the value ECLIPSE100.

The FORWARD\_MODEL keyword expects a series of keywords, each defined with
INSTALL\_JOB. The enkf will execute the jobs in sequentially in the order they
are entered. Note that the ENKF\_SCHED\_FILE keyword can be used to change the
FORWARD\_MODEL for sub-sequences of the run.

\sphinxstyleemphasis{Example A:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Suppose} \PYG{n}{that} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MY\PYGZus{}RELPERM\PYGZus{}SCRIPT}\PYG{l+s+s2}{\PYGZdq{}} \PYG{n}{has} \PYG{n}{been} \PYG{n}{defined} \PYG{k}{with}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{the} \PYG{n}{INSTALL\PYGZus{}JOB} \PYG{n}{keyword}\PYG{o}{.} \PYG{n}{This} \PYG{n}{FORWARD\PYGZus{}MODEL} \PYG{n}{will} \PYG{n}{execute}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MY\PYGZus{}RELPERM\PYGZus{}SCRIPT}\PYG{l+s+s2}{\PYGZdq{}} \PYG{n}{before} \PYG{n}{ECLIPSE100}\PYG{o}{.}
\PYG{n}{FORWARD\PYGZus{}MODEL} \PYG{n}{MY\PYGZus{}RELPERM\PYGZus{}SCRIPT} \PYG{n}{ECLIPSE100}
\end{sphinxVerbatim}

\sphinxstyleemphasis{Example B:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Suppose} \PYG{n}{that} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MY\PYGZus{}RELPERM\PYGZus{}SCRIPT}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{and} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MY\PYGZus{}ROCK\PYGZus{}PHYSICS\PYGZus{}MODEL}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{has} \PYG{n}{been} \PYG{n}{defined} \PYG{k}{with} \PYG{n}{the} \PYG{n}{INSTALL\PYGZus{}JOB} \PYG{n}{keyword}\PYG{o}{.}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{This} \PYG{n}{FORWARD\PYGZus{}MODEL} \PYG{n}{will} \PYG{n}{execute} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MY\PYGZus{}RELPERM\PYGZus{}SCRIPT}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{then}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ECLIPSE100}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{and} \PYG{o+ow}{in} \PYG{n}{the} \PYG{n}{end} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MY\PYGZus{}ROCK\PYGZus{}PHYSICS\PYGZus{}MODEL}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}
\PYG{n}{FORWARD\PYGZus{}MODEL} \PYG{n}{MY\PYGZus{}RELPERM\PYGZus{}SCRIPT} \PYG{n}{ECLIPSE100} \PYG{n}{MY\PYGZus{}ROCK\PYGZus{}PHYSICS\PYGZus{}MODEL}
\end{sphinxVerbatim}

For advanced jobs you can pass string arguments to the job using a KEY=VALUE
based approach, this is further described in: passing arguments. In available
jobs in enkf you can see a list of the jobs which are available.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:job-script}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{JOB\_SCRIPT}
\begin{quote}

Running the forward model from enkf is a multi-level process which can be
summarized as follows:
\begin{enumerate}
\item {} 
A Python module called jobs.py is written and stored in the directory where

\end{enumerate}
\end{quote}

the forward simulation is run. The jobs.py module contains a list of
job-elements, where each element is a Python representation of the code
entered when installing the job.
\begin{enumerate}
\item {} 
The enkf application submits a Python script to the enkf queue system, this

\end{enumerate}

script then loads the jobs.py module to find out which programs to run, and
how to run them.
\begin{enumerate}
\item {} 
The job\_script starts and monitors the individual jobs in the jobs.py

\end{enumerate}

module.
\begin{quote}

The JOB\_SCRIPT variable should point at the Python script which is managing
the forward model. This should normally be set in the site wide configuration
file.
\end{quote}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:queue-system}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{QUEUE\_SYSTEM}

The keyword QUEUE\_SYSTEM can be used to control where the simulation jobs are
executed. It can take the values LSF, TORQUE, RSH and LOCAL.

The LSF option will submit jobs to the LSF cluster at your location, and is
recommended whenever LSF is available.

The TORQUE option will submit jobs to the TORQUE a torque based system, using
the commands qsub, qstat etc., if available.

If you do not have access to LSF or TORQUE you can submit to your local
workstation using the LOCAL option and to homemade cluster of workstations
using the RSH option. All of the queue systems can be further configured, see
separate sections.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Tell} \PYG{n}{ert} \PYG{n}{to} \PYG{n}{use} \PYG{n}{the} \PYG{n}{LSF} \PYG{n}{cluster}\PYG{o}{.}
\PYG{n}{QUEUE\PYGZus{}SYSTEM} \PYG{n}{LSF}
\end{sphinxVerbatim}

The QUEUE\_SYSTEM keyword is optional, and usually defaults to LSF (this is
site dependent).
\end{sphinxShadowBox}


\section{Configuring LSF access}
\label{\detokenize{keywords/index:configuring-lsf-access}}\phantomsection\label{\detokenize{keywords/index:id13}}
The LSF system is the most useful of the queue alternatives, and also the
alternative with most options. The most important options are related to how ert
should submit jobs to the LSF system. Essentially there are two methods ert can
use when submitting jobs to the LSF system:
\begin{enumerate}
\item {} 
For workstations which have direct access to LSF ert can submit directly with
no further configuration. This is preferred solution, but unfortunately not
very common.

\item {} 
Alternatively ert can issue shell commands to bsub/bjobs/bkill to submit
jobs. These shell commands can be issued on the current workstation, or
alternatively on a remote workstation using ssh.

\end{enumerate}

The main switch between alternatives 1 and 2 above is the LSF\_SERVER option.

\phantomsection\label{\detokenize{keywords/index:lsf-server}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{LSF\_SERVER}

By using the LSF\_SERVER option you essentially tell ert two things about how
jobs should be submitted to LSF:
\begin{enumerate}
\item {} 
You tell ert that jobs should be submitted using shell commands.

\item {} 
You tell ert which server should be used when submitting

\end{enumerate}

So when your configuration file has the setting:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{LSF\PYGZus{}SERVER}   \PYG{n}{be}\PYG{o}{\PYGZhy{}}\PYG{n}{grid01}
\end{sphinxVerbatim}

ert will use ssh to submit your jobs using shell commands on the server
be-grid01. For this to work you must have passwordless ssh to the server
be-grid01. If you give the special server name LOCAL ert will submit using
shell commands on the current workstation.

\sphinxstylestrong{bsub/bjobs/bkill options}

By default ert will use the shell commands bsub,bjobs and bkill to interact
with the queue system, i.e. whatever binaries are first in your PATH will be
used. For fine grained control of the shell based submission you can tell ert
which programs to use:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}OPTION}   \PYG{n}{LSF}  \PYG{n}{BJOBS\PYGZus{}CMD}  \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{my}\PYG{o}{/}\PYG{n}{bjobs}
\PYG{n}{QUEUE\PYGZus{}OPTION}   \PYG{n}{LSF}  \PYG{n}{BSUB\PYGZus{}CMD}   \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{my}\PYG{o}{/}\PYG{n}{bsub}
\end{sphinxVerbatim}

\sphinxstyleemphasis{Example 1}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{LSF\PYGZus{}SERVER}    \PYG{n}{be}\PYG{o}{\PYGZhy{}}\PYG{n}{grid01}
\PYG{n}{QUEUE\PYGZus{}OPTION}  \PYG{n}{LSF}     \PYG{n}{BJOBS\PYGZus{}CMD}   \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{my}\PYG{o}{/}\PYG{n}{bjobs}
\PYG{n}{QUEUE\PYGZus{}OPTION}  \PYG{n}{LSF}     \PYG{n}{BSUB\PYGZus{}CMD}    \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{my}\PYG{o}{/}\PYG{n}{bsub}
\end{sphinxVerbatim}

In this example we tell ert to submit jobs from the workstation be-grid01
using custom binaries for bsub and bjobs.

\sphinxstyleemphasis{Example 2}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{LSF\PYGZus{}SERVER}   \PYG{n}{LOCAL}
\end{sphinxVerbatim}

In this example we will submit on the current workstation, without using ssh
first, and we will use the default bsub and bjobs executables. The remaining
LSF options apply irrespective of which method has been used to submit the
jobs.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:lsf-queue}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{LSF\_QUEUE}

The name of the LSF queue you are running ECLIPSE simulations in.
\end{sphinxShadowBox}


\section{Configuring TORQUE access}
\label{\detokenize{keywords/index:configuring-torque-access}}\phantomsection\label{\detokenize{keywords/index:id14}}
The TORQUE system is the only available system on some clusters. The most
important options are related to how ert should submit jobs to the TORQUE
system.
\begin{itemize}
\item {} 
Currently, the TORQUE option only works when the machine you are logged into
have direct access to the queue system. ert then submit directly with no
further configuration.

\end{itemize}

The most basic invocation is in other words:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}SYSTEM} \PYG{n}{TORQUE}
\end{sphinxVerbatim}

\sphinxstylestrong{qsub/qstat/qdel options}

By default ert will use the shell commands qsub,qstat and qdel to interact with
the queue system, i.e. whatever binaries are first in your PATH will be used.
For fine grained control of the shell based submission you can tell ert which
programs to use:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}SYSTEM} \PYG{n}{TORQUE}
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{QSUB\PYGZus{}CMD} \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{my}\PYG{o}{/}\PYG{n}{qsub}
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{QSTAT\PYGZus{}CMD} \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{my}\PYG{o}{/}\PYG{n}{qstat}
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{QDEL\PYGZus{}CMD} \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{my}\PYG{o}{/}\PYG{n}{qdel}
\end{sphinxVerbatim}

In this example we tell ert to submit jobs using custom binaries for bsub and
bjobs.

\sphinxstylestrong{Name of queue}

The name of the TORQUE queue you are running ECLIPSE simulations in.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{QUEUE} \PYG{n}{name\PYGZus{}of\PYGZus{}queue}
\end{sphinxVerbatim}

\sphinxstylestrong{Name of cluster (label)}

The name of the TORQUE cluster you are running ECLIPSE simulations in. This
might be a label (serveral clusters), or a single one, as in this example baloo.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{CLUSTER\PYGZus{}LABEL} \PYG{n}{baloo}
\end{sphinxVerbatim}

\sphinxstylestrong{Max running jobs}

The queue option MAX\_RUNNING controls the maximum number of simultaneous jobs
submitted to the queue when using (in this case) the TORQUE option in
QUEUE\_SYSTEM.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}SYSTEM} \PYG{n}{TORQUE}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Submit} \PYG{n}{no} \PYG{n}{more} \PYG{n}{than} \PYG{l+m+mi}{30} \PYG{n}{simultaneous} \PYG{n}{jobs}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{to} \PYG{n}{the} \PYG{n}{TORQUE} \PYG{n}{cluster}\PYG{o}{.}
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{MAX\PYGZus{}RUNNING} \PYG{l+m+mi}{30}
\end{sphinxVerbatim}

\sphinxstylestrong{Queue options controlling number of nodes and CPUs}

When using TORQUE, you must specify how many nodes a single job is should to
use, and how many CPUs per node. The default setup in ert will use one node and
one CPU. These options are called NUM\_NODES and NUM\_CPUS\_PER\_NODE.

If the numbers specified is higher than supported by the cluster (i.e. use 32
CPUs, but no node has more than 16), the job will not start.

If you wish to increase these number, the program running (typically ECLIPSE)
will usually also have to be told to correspondingly use more processing units
(keyword PARALLEL)

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}SYSTEM} \PYG{n}{TORQUE}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Use} \PYG{n}{more} \PYG{n}{nodes} \PYG{o+ow}{and} \PYG{n}{CPUs}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{o+ow}{in} \PYG{n}{the} \PYG{n}{TORQUE} \PYG{n}{cluster} \PYG{n}{per} \PYG{n}{job} \PYG{n}{submitted}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{This} \PYG{n}{should} \PYG{p}{(}\PYG{o+ow}{in} \PYG{n}{theory}\PYG{p}{)} \PYG{n}{allow} \PYG{k}{for} \PYG{l+m+mi}{24} \PYG{n}{processing}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{units} \PYG{n}{to} \PYG{n}{be} \PYG{n}{used} \PYG{n}{by} \PYG{n}{eg}\PYG{o}{.} \PYG{n}{ECLIPSE}
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{NUM\PYGZus{}NODES} \PYG{l+m+mi}{3}
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{NUM\PYGZus{}CPUS\PYGZus{}PER\PYGZus{}NODE} \PYG{l+m+mi}{8}
\end{sphinxVerbatim}

\sphinxstylestrong{Keep output from qsub}

Sometimes the error messages from qsub can be useful, if something is seriously
wrong with the environment or setup. To keep this output (stored in your home
folder), use this:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{KEEP\PYGZus{}QSUB\PYGZus{}OUTPUT} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

** Slow submit to torque **

To be more gentle with the torqueue system you can instruct the driver to sleep
for every submit request. The argument to the SUBMIT\_SLEEP is the number of
seconds to sleep for every submit, can be a fraction like 0.5.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{SUBMIT\PYGZus{}SLEEP} \PYG{l+m+mf}{0.25}
\end{sphinxVerbatim}

** Torque debug log **

You can ask the torqueu driver to store a debug log of the jobs submitted, and
the resulting job id. This is done with the queue option DEBUG\_OUTPUT:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{QUEUE\PYGZus{}OPTION} \PYG{n}{TORQUE} \PYG{n}{DEBUG\PYGZus{}OUTPUT} \PYG{n}{torque\PYGZus{}log}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}


\section{Configuring the RSH queue}
\label{\detokenize{keywords/index:configuring-the-rsh-queue}}\phantomsection\label{\detokenize{keywords/index:id15}}\phantomsection\label{\detokenize{keywords/index:rsh-host}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{RSH\_HOST}

You can run the forward model in enkf on workstations using remote-shell
commands. To use the RSH queue system you must first set a list of computers
which enkf can use for running jobs:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{RSH\PYGZus{}HOST}   \PYG{n}{computer1}\PYG{p}{:}\PYG{l+m+mi}{2}  \PYG{n}{computer2}\PYG{p}{:}\PYG{l+m+mi}{2}   \PYG{n}{large\PYGZus{}computer}\PYG{p}{:}\PYG{l+m+mi}{8}
\end{sphinxVerbatim}

Here you tell enkf that you can run on three different computers: computer1,
computer2 and large\_computer. The two first computers can accept two jobs from
enkf, and the last can take eight jobs. Observe the following when using RSH:

You must have passwordless login to the computers listed in RSH\_HOST otherwise
it will fail hard. enkf will not consider total load on the various computers;
if have said it can take two jobs, it will get two jobs, irrespective of the
existing load.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:rsh-command}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{RSH\_COMMAND}

This is the name of the executable used to invoke remote shell operations.
Will typically be either rsh or ssh. The command given to RSH\_COMMAND must
either be in PATH or an absolute path.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{MAX\PYGZus{}RUNNING\PYGZus{}RSH}
\end{sphinxVerbatim}

The keyword MAX\_RUNNING\_RSH controls the maximum number of simultaneous jobs
running when using the RSH option in QUEUE\_SYSTEM. It MAX\_RUNNING\_RSH exceeds
the total capacity defined in RSH\_HOST, it will automatically be truncated to
that capacity.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{No} \PYG{n}{more} \PYG{n}{than} \PYG{l+m+mi}{10} \PYG{n}{simultaneous} \PYG{n}{jobs}
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{running} \PYG{n}{via} \PYG{n}{RSH}\PYG{o}{.}
\PYG{n}{MAX\PYGZus{}RUNNING\PYGZus{}RSH} \PYG{l+m+mi}{10}
\end{sphinxVerbatim}
\end{sphinxShadowBox}


\section{Keywords related to plotting}
\label{\detokenize{keywords/index:keywords-related-to-plotting}}\phantomsection\label{\detokenize{keywords/index:id16}}\phantomsection\label{\detokenize{keywords/index:plot-driver}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{PLOT\_DRIVER}

This is the name of the sub system used for creating plots. The default system
is called ‘PLPLOT’ - all the other options regarding plotting are sub options
which are only relevant when you are using PLPLOT. In addition to PLPLOT you
can chose the value ‘TEXT’; this will actually not produce any plots, just
textfiles which can be used for plotting with your favorite plotting program.
This is particularly relevant if you have some special requirements to the
plots.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:plot-errorbar}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{PLOT\_ERRORBAR}

Should errorbars on the observations be plotted?
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:plot-errorbar-max}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{PLOT\_ERRORBAR\_MAX}

When plotting summary vectors for which observations have been ‘installed’
with the OBS\_CONFIG keyword, ert will plot the observed values. If you have
less than PLOT\_ERRORBAR\_MAX observations ert will use errorbars to show the
observed values, otherwise it will use two dashed lines indicating +/- one
standard deviation. This option is only meaningful when PLOT\_PLOT\_ERRORBAR is
activated.

To ensure that you always get errorbars you can set PLOT\_ERRORBAR\_MAX to a
very large value, on the other hand setting PLOT\_ERRORBAR\_MAX to 0 will ensure
that ert always plots observation uncertainty using dashed lines of +/- one
standard deviation.

The setting here will also affect the output when you are using the TEXT
driver to plot.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:plot-height}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{PLOT\_HEIGHT}

When the PLPLOT driver creates a plot file, it will have the height (in
pixels) given by the PLOT\_HEIGHT keyword. The default value for PLOT\_HEIGHT is
768 pixels.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:plot-refcase}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{PLOT\_REFCASE}

Boolean variable which is TRUE if you want to add the refcases to the plots.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{PLOT\PYGZus{}REFCASE} \PYG{n}{TRUE}
\end{sphinxVerbatim}
\end{sphinxShadowBox}

\begin{sphinxShadowBox}
\sphinxstyletopictitle{REFCASE\_LIST}

Provide one or more Eclipse .DATA files for a refcase to be added in the
plots. This refcase will be plotted in different colours. The summary files
related to the refcase should be in the same folder as the refcase.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{REFCASE\PYGZus{}LIST} \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{refcase1}\PYG{o}{/}\PYG{n}{file1}\PYG{o}{.}\PYG{n}{DATA} \PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n}{to}\PYG{o}{/}\PYG{n}{refcase2}\PYG{o}{/}\PYG{n}{file2}\PYG{o}{.}\PYG{n}{DATA}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:plot-settings}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{PLOT\_SETTINGS}

The \sphinxcode{\sphinxupquote{PLOT\_SETTINGS}} keyword is a “master keyword” which can be
used to configure some aspects of the plotting. These settings will
affect the default behaviour when you create a new plot, you can still
changes these settings interactively.

When using the \sphinxcode{\sphinxupquote{PLOT\_SETTINGS}} keyword you supply a secondary
keyword and a values as the tow arguments:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{PLOT\PYGZus{}SETTINGS} \PYG{n}{SHOW\PYGZus{}REFCASE} \PYG{k+kc}{False}
\end{sphinxVerbatim}

Will make sure that your plots are created without the refcase plotted
as default. The available secondary keys are:

SHOW\_REFCASE : Default True
SHOW\_HISTORY : Default True
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:rft-config}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{RFT\_CONFIG}

RFT\_CONFIGS argument is a file with the name of the rfts followed by date (day
month year) Ex.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{RFT\PYGZus{}CONFIG}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{/}\PYG{n}{models}\PYG{o}{/}\PYG{n}{wells}\PYG{o}{/}\PYG{n}{rft}\PYG{o}{/}\PYG{n}{WELLNAME\PYGZus{}AND\PYGZus{}RFT\PYGZus{}TIME}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

Where the contents of the file is something like

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{be}\PYG{o}{\PYGZhy{}}\PYG{n}{linapp16}\PYG{p}{(}\PYG{n}{inmyr}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{/}\PYG{n}{models}\PYG{o}{/}\PYG{n}{wells}\PYG{o}{/}\PYG{n}{rft} \PYG{l+m+mi}{34}\PYG{o}{\PYGZgt{}} \PYG{n}{more} \PYG{n}{WELLNAME\PYGZus{}AND\PYGZus{}RFT\PYGZus{}TIME}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{A}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{n}{HP}  \PYG{l+m+mi}{06} \PYG{l+m+mi}{05} \PYG{l+m+mi}{1993}
\PYG{n}{A}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{9}\PYG{n}{HW}  \PYG{l+m+mi}{31} \PYG{l+m+mi}{07} \PYG{l+m+mi}{1993}
\PYG{n}{C}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{n}{HP}  \PYG{l+m+mi}{11} \PYG{l+m+mi}{12} \PYG{l+m+mi}{2007}
\PYG{n}{C}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{n}{HP}  \PYG{l+m+mi}{21} \PYG{l+m+mi}{12} \PYG{l+m+mi}{1999}
\PYG{n}{C}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{6}\PYG{n}{HR}  \PYG{l+m+mi}{09} \PYG{l+m+mi}{11} \PYG{l+m+mi}{1999}
\PYG{n}{D}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{n}{HP}  \PYG{l+m+mi}{10} \PYG{l+m+mi}{07} \PYG{l+m+mi}{2003}
\PYG{n}{K}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{n}{HW}  \PYG{l+m+mi}{09} \PYG{l+m+mi}{02} \PYG{l+m+mi}{2003}
\PYG{n}{K}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{6}\PYG{n}{HW}  \PYG{l+m+mi}{08} \PYG{l+m+mi}{11} \PYG{l+m+mi}{2002}
\PYG{n}{K}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{7}\PYG{n}{HW}  \PYG{l+m+mi}{21} \PYG{l+m+mi}{04} \PYG{l+m+mi}{2005}
\PYG{n}{D}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{6}\PYG{n}{HP}  \PYG{l+m+mi}{22} \PYG{l+m+mi}{04} \PYG{l+m+mi}{2006}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:rftpath}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{RFTPATH}

RFTPATHs argument is the path to where the rft-files are located

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{RFTPATH}  \PYG{o}{.}\PYG{o}{.}\PYG{o}{/}\PYG{n}{models}\PYG{o}{/}\PYG{n}{wells}\PYG{o}{/}\PYG{n}{rft}\PYG{o}{/}
\end{sphinxVerbatim}
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:hook-workflow}}
With the keyword \sphinxcode{\sphinxupquote{HOOK\_WORKFLOW}} you can configure workflow ‘hooks’;
meaning workflows which will be run automatically at certain points during ERTs
execution. Currently there are four points in ERTs flow of execution where you
can hook in a workflow, before the simulations start, \sphinxcode{\sphinxupquote{PRE\_SIMULATION}};
after all the simulations have completed \sphinxcode{\sphinxupquote{POST\_SIMULATION}}; before the
update step, \sphinxcode{\sphinxupquote{PRE\_UPDATE}} and after the update step, \sphinxcode{\sphinxupquote{POST\_UPDATE}}.
The \sphinxcode{\sphinxupquote{POST\_SIMULATION}} hook is typically used to trigger QC workflows:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{HOOK\PYGZus{}WORKFLOW} \PYG{n}{initWFLOW}        \PYG{n}{PRE\PYGZus{}SIMULATION}
\PYG{n}{HOOK\PYGZus{}WORKFLOW} \PYG{n}{preUpdateWFLOW}   \PYG{n}{PRE\PYGZus{}UPDATE}
\PYG{n}{HOOK\PYGZus{}WORKFLOW} \PYG{n}{postUpdateWFLOW}  \PYG{n}{POST\PYGZus{}UPDATE}
\PYG{n}{HOOK\PYGZus{}WORKFLOW} \PYG{n}{QC\PYGZus{}WFLOW1}        \PYG{n}{POST\PYGZus{}SIMULATION}
\PYG{n}{HOOK\PYGZus{}WORKFLOW} \PYG{n}{QC\PYGZus{}WFLOW2}        \PYG{n}{POST\PYGZus{}SIMULATION}
\end{sphinxVerbatim}

In this example the workflow \sphinxcode{\sphinxupquote{initWFLOW}} will run after all the simulation
directories have been created, just before the forward model is submitted to the
queue. The workflow \sphinxcode{\sphinxupquote{preUpdateWFLOW}} will be run before the update step
and \sphinxcode{\sphinxupquote{postUpdateWFLOW}} will be run after the update step. When all the
simulations are complete the two workflows \sphinxcode{\sphinxupquote{QC\_WFLOW1}} and
\sphinxcode{\sphinxupquote{QC\_WFLOW2}} will be run.

Observe that the workflows being ‘hooked in’ with the \sphinxcode{\sphinxupquote{HOOK\_WORKFLOW}} must
be loaded with the \sphinxcode{\sphinxupquote{LOAD\_WORKFLOW}} keyword.

Currently, \sphinxcode{\sphinxupquote{PRE\_UPDATE}} and \sphinxcode{\sphinxupquote{POST\_UPDATE}} are only available from
python.


\section{Manipulating the Unix environment}
\label{\detokenize{keywords/index:manipulating-the-unix-environment}}\phantomsection\label{\detokenize{keywords/index:id17}}
The two keywords SETENV and UPDATE\_PATH can be used to manipulate the Unix
environment of the ERT process, tha manipulations only apply to the running ERT
instance, and are not applied to the shell.

\phantomsection\label{\detokenize{keywords/index:setenv}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{SETENV}

You can use the SETENV keyword to alter the unix environment enkf is running
in. This is probably most relevant for setting up the environment for the
external jobs invoked by enkf.

\sphinxstyleemphasis{Example:}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}} \PYG{n}{Setting} \PYG{n}{up} \PYG{n}{LSF}
\PYG{n}{SETENV}  \PYG{n}{LSF\PYGZus{}BINDIR}      \PYG{o}{/}\PYG{n}{prog}\PYG{o}{/}\PYG{n}{LSF}\PYG{o}{/}\PYG{l+m+mf}{7.0}\PYG{o}{/}\PYG{n}{linux2}\PYG{o}{.}\PYG{l+m+mi}{6}\PYG{o}{\PYGZhy{}}\PYG{n}{glibc2}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{o}{\PYGZhy{}}\PYG{n}{x86\PYGZus{}64}\PYG{o}{/}\PYG{n+nb}{bin}
\PYG{n}{SETENV}  \PYG{n}{LSF\PYGZus{}LIBDIR}      \PYG{o}{/}\PYG{n}{prog}\PYG{o}{/}\PYG{n}{LSF}\PYG{o}{/}\PYG{l+m+mf}{7.0}\PYG{o}{/}\PYG{n}{linux2}\PYG{o}{.}\PYG{l+m+mi}{6}\PYG{o}{\PYGZhy{}}\PYG{n}{glibc2}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{o}{\PYGZhy{}}\PYG{n}{x86\PYGZus{}64}\PYG{o}{/}\PYG{n}{lib}
\PYG{n}{SETENV}  \PYG{n}{LSF\PYGZus{}UIDDIR}      \PYG{o}{/}\PYG{n}{prog}\PYG{o}{/}\PYG{n}{LSF}\PYG{o}{/}\PYG{l+m+mf}{7.0}\PYG{o}{/}\PYG{n}{linux2}\PYG{o}{.}\PYG{l+m+mi}{6}\PYG{o}{\PYGZhy{}}\PYG{n}{glibc2}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{o}{\PYGZhy{}}\PYG{n}{x86\PYGZus{}64}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{uid}
\PYG{n}{SETENV}  \PYG{n}{LSF\PYGZus{}SERVERDIR}   \PYG{o}{/}\PYG{n}{prog}\PYG{o}{/}\PYG{n}{LSF}\PYG{o}{/}\PYG{l+m+mf}{7.0}\PYG{o}{/}\PYG{n}{linux2}\PYG{o}{.}\PYG{l+m+mi}{6}\PYG{o}{\PYGZhy{}}\PYG{n}{glibc2}\PYG{o}{.}\PYG{l+m+mi}{3}\PYG{o}{\PYGZhy{}}\PYG{n}{x86\PYGZus{}64}\PYG{o}{/}\PYG{n}{etc}
\PYG{n}{SETENV}  \PYG{n}{LSF\PYGZus{}ENVDIR}      \PYG{o}{/}\PYG{n}{prog}\PYG{o}{/}\PYG{n}{LSF}\PYG{o}{/}\PYG{n}{conf}
\end{sphinxVerbatim}

Observe that the SETENV command is not as powerful as the corresponding shell
utility. In particular you can not use \$VAR to refer to the existing value of
an environment variable. To add elements to the PATH variable it is easier to
use the UPDATE\_PATH keyword.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:update-path}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{UPDATE\_PATH}

The UPDATE\_PATH keyword will prepend a new element to an existing PATH
variable. I.e. the config

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{UPDATE\PYGZus{}PATH}   \PYG{n}{PATH}  \PYG{o}{/}\PYG{n}{some}\PYG{o}{/}\PYG{n}{funky}\PYG{o}{/}\PYG{n}{path}\PYG{o}{/}\PYG{n+nb}{bin}
\end{sphinxVerbatim}

will be equivalent to the shell command:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
setenv PATH /some/funky/path/bin:\PYGZdl{}PATH
\end{sphinxVerbatim}

The whole thing is just a workaround because we can not use \$PATH.
\end{sphinxShadowBox}
\phantomsection\label{\detokenize{keywords/index:update-settings}}
The \sphinxcode{\sphinxupquote{UPDATE\_SETTINGS}} keyword is a \sphinxstyleemphasis{super-keyword} which can be used to
control parameters which apply to the Ensemble Smoother update algorithm. The
:code:{\color{red}\bfseries{}{}`}UPDATE\_SETTINGS{}`currently supports the two subkeywords:
\begin{quote}
\begin{description}
\item[{OVERLAP\_LIMIT Scaling factor used when detecting outliers. Increasing this}] \leavevmode
factor means that more observations will potentially be included in the
assimilation. The default value is 3.00..

Including outliers in the Smoother algorithm can dramatically increase the
coupling between the ensemble members. It is therefore important to filter out
these outlier data prior to data assimilation. An observation, textstyle
d\textasciicircum{}o\_i, will be classified as an outlier if

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{\textbar{}}\PYG{n}{d}\PYG{o}{\PYGZca{}}\PYG{n}{o\PYGZus{}i} \PYG{o}{\PYGZhy{}} \PYGZbs{}\PYG{n}{bar}\PYG{p}{\PYGZob{}}\PYG{n}{d}\PYG{p}{\PYGZcb{}}\PYG{n}{\PYGZus{}i}\PYG{o}{\textbar{}} \PYG{o}{\PYGZgt{}} \PYGZbs{}\PYG{n}{mathrm}\PYG{p}{\PYGZob{}}\PYG{n}{ENKF}\PYGZbs{}\PYG{n}{\PYGZus{}ALPHA}\PYG{p}{\PYGZcb{}} \PYGZbs{}\PYG{n}{left}\PYG{p}{(}\PYG{n}{s\PYGZus{}}\PYG{p}{\PYGZob{}}\PYG{n}{d\PYGZus{}i}\PYG{p}{\PYGZcb{}} \PYG{o}{+} \PYGZbs{}\PYG{n}{sigma\PYGZus{}}\PYG{p}{\PYGZob{}}\PYG{n}{d}\PYG{o}{\PYGZca{}}\PYG{n}{o\PYGZus{}i}\PYG{p}{\PYGZcb{}}\PYGZbs{}\PYG{n}{right}\PYG{p}{)}\PYG{p}{,}
\end{sphinxVerbatim}

where textstyleboldsymbol\{d\}\textasciicircum{}o is the vector of observed data,
textstyleboldsymbol\{bar\{d\}\} is the average of the forcasted data ensemble,
textstyleboldsymbol\{s\_\{d\}\} is the vector of estimated standard deviations
for the forcasted data ensemble, and textstyleboldsymbol\{s\_\{d\}\textasciicircum{}o\} is the
vector standard deviations for the observation error (specified a priori).

\item[{STD\_CUTOFF If the ensemble variation for one particular measurment is below}] \leavevmode
this limit the observation will be deactivated. he default value for
this cutoff is 1e-6.

\end{description}
\end{quote}

Observe that for the updates many settings should be applied on the analysis
module in question.

\phantomsection\label{\detokenize{keywords/index:umask}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{UMASK}

The \sphinxtitleref{umask} is a concept used by Linux to controll the permissions on
newly created files. By default the files created by ert will have the
default permissions of your account, but by using the keyword \sphinxtitleref{UMASK}
you can alter the permissions of files created by ert.

To determine the initial permissions on newly created files start with
the initial permissions \sphinxtitleref{-rw-rw-rw-} (octal 0666) for files and
\sphinxtitleref{-rwxrwxrwx} (octal 0777) for directories, and then \sphinxstyleemphasis{\textasciitilde{}subtract} the
current umask setting. So if you wish the newly created files to have
permissions \sphinxtitleref{-rw-r—\textendash{}} you need to subtract write permissions for
group and read and write permissions for others - corresponding to
\sphinxtitleref{umask 0026}.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{UMASK} \PYG{l+m+mi}{0022}
\end{sphinxVerbatim}

We remove write permissions from group and others, implying that
everyone can read the files and directories created by ert, but only the
owner can write to them. Also everyone can execute the directories (i.e.
list the content).

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{UMASK} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}

No permissions are removed, i.e. everyone can do everything with the
files and directories created by ert.

The umask setting in ert is passed on to the forward model, and should
apply to the files/directories created by the forward model also.
However - the executables in the forward model can in principle set it’s
own umask setting or alter permissions in another way - so there is no
guarantee that the umask setting will apply to all files created by the
forward model.

The octal permissions are based on three octal numbers for owner, group
and others, where each value is based on adding the constants:
\begin{quote}

1: Execute permission
2: Write permission
4: Read permission
\end{quote}

So an octal permission of 0754 means:
\begin{itemize}
\item {} 
Owner(7) can execute(1), write(2) and read(4).

\item {} 
Group(5) can execute(1) and read(4).

\item {} 
Others(2) can read(4)

\end{itemize}
\end{sphinxShadowBox}


\chapter{Release notes for ERT}
\label{\detokenize{changes/index:release-notes-for-ert}}\label{\detokenize{changes/index::doc}}

\section{Version 2.3}
\label{\detokenize{changes/index:version-2-3}}

\subsection{2.3 ert application}
\label{\detokenize{changes/index:ert-application}}
PR: 67 - 162


\subsection{2.3 libres}
\label{\detokenize{changes/index:libres}}
PR: 105 - 411


\subsection{2.3 libecl}
\label{\detokenize{changes/index:libecl}}
PR: 170 - 506


\section{Version 2.2}
\label{\detokenize{changes/index:version-2-2}}

\subsection{2.2: ert application}
\label{\detokenize{changes/index:id1}}
Version 2.2.1 September 2017 PR: 1 - 66
Cherry-picked: \sphinxhref{https://github.com/Statoil/ert/pull/70/}{70}

Misc:
\begin{itemize}
\item {} 
Using res\_config changes from libres \sphinxhref{https://github.com/Statoil/ert/pull/16/}{{[}16{]}}

\item {} 
files moved from libecl to libres: \sphinxhref{https://github.com/Statoil/ert/pull/51}{{[}51{]}}

\item {} 
replaced ert.enkf with res.enkf \sphinxhref{https://github.com/Statoil/ert/pull/56/}{{[}56{]}}

\item {} 
Created ErtVersion: {[}\sphinxhref{https://github.com/Statoil/ert/pull/61/}{61}, \sphinxhref{https://github.com/Statoil/ert/pull/66/}{66}{]}.

\item {} 
Using res\_config: {[}\sphinxhref{https://github.com/Statoil/ert/pull/62/}{62}{]}

\item {} 
Removed dead workflow files: \sphinxhref{https://github.com/Statoil/ert/pull/64/}{{[}64{]}}

\end{itemize}

Build and testing:
\begin{itemize}
\item {} 
Cleanup after repo split {[}\sphinxhref{https://github.com/Statoil/ert/pull/1/}{1}, \sphinxhref{https://github.com/Statoil/ert/pull/2/}{2}, \sphinxhref{https://github.com/Statoil/ert/pull/3/}{3} , \sphinxhref{https://github.com/Statoil/ert/pull/4/}{4}, \sphinxhref{https://github.com/Statoil/ert/pull/5/}{5} , \sphinxhref{https://github.com/Statoil/ert/pull/6/}{6}{]}

\item {} 
Added test\_install functionality {[}\sphinxhref{https://github.com/Statoil/ert/pull/7/}{7}{]}

\item {} 
Added travis build script for libecl+libres+ert:
{[}\sphinxhref{https://github.com/Statoil/ert/pull/15/}{15}, \sphinxhref{https://github.com/Statoil/ert/pull/17/}{17}, \sphinxhref{https://github.com/Statoil/ert/pull/18/}{18}, \sphinxhref{https://github.com/Statoil/ert/pull/19/}{19}, \sphinxhref{https://github.com/Statoil/ert/pull/21/}{21}, \sphinxhref{https://github.com/Statoil/ert/pull/26/}{26}, \sphinxhref{https://github.com/Statoil/ert/pull/27/}{27}, \sphinxhref{https://github.com/Statoil/ert/pull/39/}{39,} \sphinxhref{https://github.com/Statoil/ert/pull/52/}{52}-\sphinxhref{https://github.com/Statoil/ert/pull/55/}{55}, \sphinxhref{https://github.com/Statoil/ert/pull/63/}{63}{]}

\item {} 
MacOS build error: {[}\sphinxhref{https://github.com/Statoil/ert/pull/28/}{28}{]}.

\item {} 
Created simple gui\_test bin/gui\_test {[}\sphinxhref{https://github.com/Statoil/ert/pull/32/}{32}{]}

\item {} 
cmake - create symlink: {[}\sphinxhref{https://github.com/Statoil/ert/pull/41/}{41}, \sphinxhref{https://github.com/Statoil/ert/pull/42/}{42}, \sphinxhref{https://github.com/Statoil/ert/pull/43/}{43}{]}

\item {} 
Initial Python3 testing {[}\sphinxhref{https://github.com/Statoil/ert/pull/58/}{58}, \sphinxhref{https://github.com/Statoil/ert/pull/60/}{60}{]}.

\end{itemize}

Queue and running:
\begin{itemize}
\item {} 
Added base run model - gui model updates: {[}\sphinxhref{https://github.com/Statoil/ert/pull/20/}{20}{]}.

\item {} 
Added single simulation pretest running {[}\sphinxhref{https://github.com/Statoil/ert/pull/33/}{33}, \sphinxhref{https://github.com/Statoil/ert/pull/36/}{36}, \sphinxhref{https://github.com/Statoil/ert/pull/50/}{50}, \sphinxhref{https://github.com/Statoil/ert/pull/67/}{67}{]}.

\item {} 
Add run\_id to simulation batches.

\end{itemize}


\subsection{2.2: libres}
\label{\detokenize{changes/index:id40}}
Version 2.2.9 September 2017 PR: 1 - 104
Cherry-picks: {[}\sphinxhref{https://github.com/Statoil/res/pull/106/}{106}, \sphinxhref{https://github.com/Statoil/res/pull/108/}{108}, \sphinxhref{https://github.com/Statoil/res/pull/110/}{110}, \sphinxhref{https://github.com/Statoil/res/pull/118/}{118}, \sphinxhref{https://github.com/Statoil/res/pull/121/}{121}, \sphinxhref{https://github.com/Statoil/res/pull/122/}{122}, \sphinxhref{https://github.com/Statoil/res/pull/123/}{123}, \sphinxhref{https://github.com/Statoil/res/pull/127/}{127}{]}

Misc:
\begin{itemize}
\item {} 
implement legacy from ert.xxx {[}\sphinxhref{https://github.com/Statoil/res/pull/1/}{1,} \sphinxhref{https://github.com/Statoil/res/pull/20/}{20,} \sphinxhref{https://github.com/Statoil/res/pull/21/}{21,} \sphinxhref{https://github.com/Statoil/res/pull/22/}{22}{]}

\item {} 
Setting up libres\_util and moving ert\_log there {[}\sphinxhref{https://github.com/Statoil/res/pull/13/}{13}, \sphinxhref{https://github.com/Statoil/res/pull/44/}{44}, \sphinxhref{https://github.com/Statoil/res/pull/48/}{48}{]}.

\item {} 
Added subst\_list + block\_fs functionality to res\_util - moved from
libecl {[}\sphinxhref{https://github.com/Statoil/res/pull/27/}{27}, \sphinxhref{https://github.com/Statoil/res/pull/68/}{68}, \sphinxhref{https://github.com/Statoil/res/pull/74/}{74}{]}.

\item {} 
Do not generate parameters.txt if no GEN\_KW is specified.{[}\sphinxhref{https://github.com/Statoil/res/pull/89/}{89}{]}

\item {} 
Started using RES\_VERSION {[}\sphinxhref{https://github.com/Statoil/res/pull/91/}{91}{]}.

\item {} 
CONFIG\_PATH subtitution settings - bug fixed{[}\sphinxhref{https://github.com/Statoil/res/pull/43/}{43}, \sphinxhref{https://github.com/Statoil/res/pull/96/}{96}{]}.

\item {} 
Will load summary if GEN\_DATA is present {[}\sphinxhref{https://github.com/Statoil/res/pull/123/}{123}, \sphinxhref{https://github.com/Statoil/res/pull/127/}{127}{]}

\end{itemize}

Build and test fixes:
\begin{itemize}
\item {} 
Simple functionality to do post-install testing{[}\sphinxhref{https://github.com/Statoil/res/pull/3/}{3}{]}

\item {} 
Use libecl as cmake target{[}\sphinxhref{https://github.com/Statoil/res/pull/6/}{6},{}`15 \textless{}\sphinxurl{https://github.com/Statoil/res/pull/15/}\textgreater{}{}`\_{]}

\item {} 
removed stale binaries {[}\sphinxhref{https://github.com/Statoil/res/pull/7/}{7}, \sphinxhref{https://github.com/Statoil/res/pull/9/}{9}{]}

\item {} 
travis will build all repositories {[}\sphinxhref{https://github.com/Statoil/res/pull/23/}{23}{]}.

\item {} 
Travis + OSX {[}\sphinxhref{https://github.com/Statoil/res/pull/69/}{69}, \sphinxhref{https://github.com/Statoil/res/pull/72/}{72}{]}

\item {} 
Remove statoil specific settings from build sytem {[}\sphinxhref{https://github.com/Statoil/res/pull/38/}{38}{]}.

\item {} 
Travis split for parallell builds {[}\sphinxhref{https://github.com/Statoil/res/pull/79/}{79}{]}.

\end{itemize}

Config refactor:
\begin{quote}

In this release cycle there have been large amount of changes to the
code configuring the ERT state; the purpose of these changes has
been to prepare for further development with Everest. The main net
change is that a new configuration object - res\_config has been
created ,which holds all the configuration subobjects:
\begin{quote}

{[}\sphinxhref{https://github.com/Statoil/res/pull/10/}{10}, \sphinxhref{https://github.com/Statoil/res/pull/14/}{14}, \sphinxhref{https://github.com/Statoil/res/pull/35/}{35}, \sphinxhref{https://github.com/Statoil/res/pull/39/}{39}, \sphinxhref{https://github.com/Statoil/res/pull/45/}{45}, \sphinxhref{https://github.com/Statoil/res/pull/52/}{52}, \sphinxhref{https://github.com/Statoil/res/pull/54/}{54}, \sphinxhref{https://github.com/Statoil/res/pull/58/}{58}-\sphinxhref{https://github.com/Statoil/res/pull/62/}{62}, \sphinxhref{https://github.com/Statoil/res/pull/66/}{66}, \sphinxhref{https://github.com/Statoil/res/pull/75/}{75}{]}
\end{quote}
\end{quote}

Queue layer:
{}`
\begin{quote}
\begin{itemize}
\item {} 
Improved logging {[}\sphinxhref{https://github.com/Statoil/res/pull/17/}{17}, \sphinxhref{https://github.com/Statoil/res/pull/37/}{37}{]}.

\item {} 
Funcionality to create a queue\_config object copy {[}\sphinxhref{https://github.com/Statoil/res/pull/36/}{36}{]}.

\end{itemize}

As part of this development cycle the job\_dispatch script has been
included in the libres distribution. There are many PR’s related to
this script:
\begin{quote}
\begin{description}
\item[{{[}\sphinxhref{https://github.com/Statoil/res/pull/28/}{28}, \sphinxhref{https://github.com/Statoil/res/pull/40/}{40}, \sphinxhref{https://github.com/Statoil/res/pull/1/}{41}, \sphinxhref{https://github.com/Statoil/res/pull/51/}{51}, \sphinxhref{https://github.com/Statoil/res/pull/53/}{53}, \sphinxhref{https://github.com/Statoil/res/pull/63/}{63}, \sphinxhref{https://github.com/Statoil/res/pull/64/}{64}, \sphinxhref{https://github.com/Statoil/res/pull/83/}{83}, \sphinxhref{https://github.com/Statoil/res/pull/84/}{84}, \sphinxhref{https://github.com/Statoil/res/pull/85/}{85}, \sphinxhref{https://github.com/Statoil/res/pull/93/}{93}, \sphinxhref{https://github.com/Statoil/res/pull/94/}{94}, \sphinxhref{https://github.com/Statoil/res/pull/95/}{95}, \sphinxhref{https://github.com/Statoil/res/pull/97/}{97}-\sphinxhref{https://github.com/Statoil/res/pull/99/}{99},}] \leavevmode
\sphinxhref{https://github.com/Statoil/res/pull/101/}{101}, \sphinxhref{https://github.com/Statoil/res/pull/103/}{103}, \sphinxhref{https://github.com/Statoil/res/pull/108/}{108}, \sphinxhref{https://github.com/Statoil/res/pull/110/}{110}{]}

\end{description}
\end{quote}
\begin{itemize}
\item {} 
Create a common run\_id for one batch of simulations, and generally
treat one batch of simulations as one unit, in a better way than
previously: {[}\sphinxhref{https://github.com/Statoil/res/pull/42/}{42}, \sphinxhref{https://github.com/Statoil/res/pull/67/}{67}{]}

\item {} 
Added PPU (Paay Per Use) code to LSF driver {[}\sphinxhref{https://github.com/Statoil/res/pull/71/}{71}{]}.

\item {} 
Workflow job PRE\_SIMULATION\_COPY {[}\sphinxhref{https://github.com/Statoil/res/pull/73/}{73}, \sphinxhref{https://github.com/Statoil/res/pull/88/}{88}{]}.

\item {} 
Allow to unset QUEUE\_OPTION {[}\sphinxhref{https://github.com/Statoil/res/pull/87/}{87}{]}.

\item {} 
Jobs failing due to dead nodes are restarted {[}\sphinxhref{https://github.com/Statoil/res/pull/100/}{100}{]}.

\end{itemize}
\end{quote}

Documentation:
\begin{itemize}
\item {} 
Formatting bugs: {[}\sphinxhref{https://github.com/Statoil/res/pull/49/}{49}, \sphinxhref{https://github.com/Statoil/res/pull/50/}{50}{]}

\item {} 
Removed doxygen + build rst {[}\sphinxhref{https://github.com/Statoil/res/pull/29/}{29}{]}

\end{itemize}


\subsection{2.2: libecl}
\label{\detokenize{changes/index:id117}}
Version 2.2.0 September 2017 PR: 1 - 169
Open PR: 108, 145

Grid:
\begin{itemize}
\item {} 
Extracted implementation ecl\_nnc\_geometry {[}\sphinxhref{https://github.com/Statoil/libecl/pull/1/}{1}, \sphinxhref{https://github.com/Statoil/libecl/pull/66/}{66}, \sphinxhref{https://github.com/Statoil/libecl/pull/75/}{75}, \sphinxhref{https://github.com/Statoil/libecl/pull/78/}{78}, \sphinxhref{https://github.com/Statoil/libecl/pull/80/}{80}, \sphinxhref{https://github.com/Statoil/libecl/pull/109/}{109}{]}.

\item {} 
Fix bug in cell\_contains for mirrored grid {[}\sphinxhref{https://github.com/Statoil/libecl/pull/51/}{51}, \sphinxhref{https://github.com/Statoil/libecl/pull/53/}{53}{]}.

\item {} 
Extract subgrid from grid {[}\sphinxhref{https://github.com/Statoil/libecl/pull/56/}{56}{]}.

\item {} 
Expose mapaxes {[}\sphinxhref{https://github.com/Statoil/libecl/pull/63/}{63}, \sphinxhref{https://github.com/Statoil/libecl/pull/64/}{64}{]}.

\item {} 
grid.get\_lgr - numbered lookup {[}\sphinxhref{https://github.com/Statoil/libecl/pull/83/}{83}{]}

\item {} 
Added NUMRES values to EGRID header {[}\sphinxhref{https://github.com/Statoil/libecl/pull/125/}{125}{]}.

\end{itemize}

Build \& testing:
\begin{quote}
\begin{itemize}
\item {} 
Removed warnings - added pylint {[}\sphinxhref{https://github.com/Statoil/libecl/pull/4/}{4}, \sphinxhref{https://github.com/Statoil/libecl/pull/5/}{5}, \sphinxhref{https://github.com/Statoil/libecl/pull/6/}{6}, \sphinxhref{https://github.com/Statoil/libecl/pull/10/}{10}, \sphinxhref{https://github.com/Statoil/libecl/pull/11/}{11}, \sphinxhref{https://github.com/Statoil/libecl/pull/12/}{12}{]}

\item {} 
Accept any Python 2.7.x version {[}\sphinxhref{https://github.com/Statoil/libecl/pull/17/}{17}, \sphinxhref{https://github.com/Statoil/libecl/pull/18/}{18}{]}

\item {} 
Remove ERT testing \& building {[}\sphinxhref{https://github.com/Statoil/libecl/pull/3/}{3}, \sphinxhref{https://github.com/Statoil/libecl/pull/19/}{19}{]}

\item {} 
Changes to Python/cmake machinery {[}\sphinxhref{https://github.com/Statoil/libecl/pull/25/}{25}, \sphinxhref{https://github.com/Statoil/libecl/pull/3/}{30}, \sphinxhref{https://github.com/Statoil/libecl/pull/31/}{31}, \sphinxhref{https://github.com/Statoil/libecl/pull/32/}{32}, \sphinxhref{https://github.com/Statoil/libecl/pull/49/}{49}, \sphinxhref{https://github.com/Statoil/libecl/pull/52/}{52}, \sphinxhref{https://github.com/Statoil/libecl/pull/62/}{62}{]}.

\item {} 
Added cmake config file {[}\sphinxhref{https://github.com/Statoil/libecl/pull/33/}{33}, \sphinxhref{https://github.com/Statoil/libecl/pull/44/}{44}, \sphinxhref{https://github.com/Statoil/libecl/pull/45/}{45}, \sphinxhref{https://github.com/Statoil/libecl/pull/47/}{47}{]}.

\item {} 
Only \sphinxstyleemphasis{one} library {[}\sphinxhref{https://github.com/Statoil/libecl/pull/54/}{54}, \sphinxhref{https://github.com/Statoil/libecl/pull/55/}{55}, \sphinxhref{https://github.com/Statoil/libecl/pull/58/}{58},

\end{itemize}

\sphinxhref{https://github.com/Statoil/libecl/pull/69/}{69}, \sphinxhref{https://github.com/Statoil/libecl/pull/73/}{73}, \sphinxhref{https://github.com/Statoil/libecl/pull/77/}{77}, \sphinxhref{https://github.com/Statoil/libecl/pull/91/}{91}, \sphinxhref{https://github.com/Statoil/libecl/pull/133/}{133}{]}
- Removed stale binaries {[}\sphinxhref{https://github.com/Statoil/libecl/pull/59/}{59}{]}.
- Require cmake \textgreater{}= 2.8.12 {[}\sphinxhref{https://github.com/Statoil/libecl/pull/67/}{67}{]}.
- Fix build on OSX {[}\sphinxhref{https://github.com/Statoil/libecl/pull/87/}{87}, \sphinxhref{https://github.com/Statoil/libecl/pull/88/}{88}, \sphinxhref{https://github.com/Statoil/libecl/pull/95/}{95}, \sphinxhref{https://github.com/Statoil/libecl/pull/103/}{103}{]}.
- Fix broken behavior with internal test data {[}\sphinxhref{https://github.com/Statoil/libecl/pull/97/}{97}{]}.
- Travis - compile with -Werror {[}\sphinxhref{https://github.com/Statoil/libecl/pull/122/}{122}, \sphinxhref{https://github.com/Statoil/libecl/pull/123/}{123}, \sphinxhref{https://github.com/Statoil/libecl/pull/127/}{127}, \sphinxhref{https://github.com/Statoil/libecl/pull/130/}{130}{]}
- Started to support Python3 syntax {[}\sphinxhref{https://github.com/Statoil/libecl/pull/150/}{150}, \sphinxhref{https://github.com/Statoil/libecl/pull/161/}{161}{]}
- Add support for paralell builds on Travis {[}\sphinxhref{https://github.com/Statoil/libecl/pull/149/}{149}{]}
\end{quote}

libecl now fully supports OSX. On Travis it is compiled with
-Werror=all which should protect against future warnings.

C++:
\begin{itemize}
\item {} 
Removed use of deignated initializers {[}\sphinxhref{https://github.com/Statoil/libecl/pull/7/}{7}{]}.

\item {} 
Memory leak in EclFilename.cpp {[}\sphinxhref{https://github.com/Statoil/libecl/pull/14/}{14}{]}.

\item {} 
Guarantee C linkage for ecl\_data\_type {[}\sphinxhref{https://github.com/Statoil/libecl/pull/65/}{65}{]}.

\item {} 
New smspec overload {[}\sphinxhref{https://github.com/Statoil/libecl/pull/89/}{89}{]}.

\item {} 
Use -std=c++0x if -std=c++11 is unavailable {[}\sphinxhref{https://github.com/Statoil/libecl/pull/118/}{118}{]}

\item {} 
Make all of (previous( libutil compile with C++ {[}\sphinxhref{https://github.com/Statoil/libecl/pull/162/}{162}{]}

\end{itemize}

Well:
\begin{itemize}
\item {} 
Get well rates from restart files {[}\sphinxhref{https://github.com/Statoil/libecl/pull/8/}{8},{}`20 \textless{}\sphinxurl{https://github.com/Statoil/res/pull/20/}\textgreater{}{}`\_{]}.

\item {} 
Test if file exists before load {[}\sphinxhref{https://github.com/Statoil/libecl/pull/111/}{111}{]}.

\item {} 
Fix some warnings {[}\sphinxhref{https://github.com/Statoil/libecl/pull/169/}{169}{]}

\end{itemize}

Core:
\begin{itemize}
\item {} 
Support for variable length strings in binary eclipse files {[}\sphinxhref{https://github.com/Statoil/libecl/pull/13/}{13}, \sphinxhref{https://github.com/Statoil/libecl/pull/146/}{146}{]}.

\item {} 
Renamed root package ert -\textgreater{} ecl {[}\sphinxhref{https://github.com/Statoil/libecl/pull/21/}{21}{]}

\item {} 
Load INTERSECT summary files with NAMES instead WGNAMES {[}\sphinxhref{https://github.com/Statoil/libecl/pull/34/}{34} - \sphinxhref{https://github.com/Statoil/libecl/pull/39/}{39}{]}.

\item {} 
Possible memory leak: {[}\sphinxhref{https://github.com/Statoil/libecl/pull/61/}{61}{]}

\item {} 
Refactored binary time search in \_\_get\_index\_from\_sim\_time() {[}\sphinxhref{https://github.com/Statoil/libecl/pull/113/}{113}{]}

\item {} 
Possible to mark fortio writer as “failed” - will unlink on close {[}\sphinxhref{https://github.com/Statoil/libecl/pull/119/}{119}{]}.

\item {} 
Allow keywords of more than 8 characters {[}\sphinxhref{https://github.com/Statoil/libecl/pull/120/}{120}, \sphinxhref{https://github.com/Statoil/libecl/pull/124/}{124}{]}.

\item {} 
ecl\_sum writer: Should write RESTART keyword {[}\sphinxhref{https://github.com/Statoil/libecl/pull/129/}{129}, \sphinxhref{https://github.com/Statoil/libecl/pull/131/}{131}{]}

\item {} 
Made EclVersion class {[}\sphinxhref{https://github.com/Statoil/libecl/pull/160/}{160}{]}

\item {} 
Functionality to dump an index file for binary files: {[}\sphinxhref{https://github.com/Statoil/libecl/pull/155/}{155}, \sphinxhref{https://github.com/Statoil/libecl/pull/159/}{159}, \sphinxhref{https://github.com/Statoil/libecl/pull/163/}{163}, \sphinxhref{https://github.com/Statoil/libecl/pull/166/}{166}, \sphinxhref{https://github.com/Statoil/libecl/pull/167/}{167}{]}

\end{itemize}

Misc:
\begin{itemize}
\item {} 
Added legacy pacakge ert/ {[}\sphinxhref{https://github.com/Statoil/libecl/pull/48/}{48}, \sphinxhref{https://github.com/Statoil/libecl/pull/99/}{99}{]}

\item {} 
Improved logging - adding enums for og levels {[}\sphinxhref{https://github.com/Statoil/libecl/pull/90/}{90}, \sphinxhref{https://github.com/Statoil/libecl/pull/140/}{140}, \sphinxhref{https://github.com/Statoil/libecl/pull/141/}{141}{]}

\item {} 
Refactored to use snake\_case instead of CamelCase {[}\sphinxhref{https://github.com/Statoil/libecl/pull/144/}{144}, \sphinxhref{https://github.com/Statoil/libecl/pull/145/}{145}{]}

\end{itemize}


\bigskip\hrule\bigskip


Version 2.1.0 February 2017  PR: 1150 - 1415
Open PR: 1352, 1358, 1362

Queue system/workflows:
\begin{itemize}
\item {} 
Functionality to blacklist nodes from LSF {[}1240, 1256, 1258, 1274, 1412, 1415{]}.

\item {} 
Use bhist command to check lsf job status if job has expired from bjobs {[}1301{]}.

\item {} 
Debug output from torque goes to stdout {[}1151{]}.

\item {} 
Torque driver will not abort if qstat returns invalid status {[}1411{]}.

\item {} 
Simulation status USER\_EXIT - count as failed {[}1166{]}.

\item {} 
Added Enum identifier ‘JOB\_QUEUE\_DO\_KILL\_NODE\_FAILURE {[}1268{]}.

\item {} 
Have deprecated the ability to set queue options directly on the drivers {[}1288{]}.

\item {} 
Added system for version specific loading for workflow job model
description files {[}1177{]}.

\item {} 
Job loader should not try to load from directory {[}1187{]}.

\item {} 
Refactoring of max runtime - initiated from WPRO {[}1237{]}.

\item {} 
Determine which nodes are running a job {[}1251{]}.

\end{itemize}

Build updates:
\begin{itemize}
\item {} 
Check if python modules are present in the required version {[}1150{]}.

\item {} 
Do not build ERT\_GUI if PyQt4 is not found {[}1150, 1230{]}.

\item {} 
Do not fail build numpy is not found {[}1153{]}.

\item {} 
Allow for user provided CMAKE\_C\_FLAGS on linux {[}1300{]}.

\item {} 
Require exactly version 2.7 of Python {[}1307{]}.

\item {} 
Travis testing improvements {[}1363{]}.

\item {} 
Removed devel/ directory from source {[}1196{]}.

\item {} 
Setting correct working directory, and build target dependency
for sphinx-apidoc / html generation {[}1385{]}.

\end{itemize}

Eclipse library:
\begin{itemize}
\item {} 
C++ move constructor and operator= for smspec\_node {[}1155, 1200{]}.

\item {} 
fortio\_ftruncate( ) {[}1161{]}.

\item {} 
INIT writer will write keywords DEPTH, DX, DY, DZ {[}1164, 1172, 1311, 1388{]}.

\item {} 
Grid writer will take unit system enum argument {[}1164{]}.

\item {} 
New function ecl\_kw\_first\_different( ) {[}1165{]}.

\item {} 
Completion variables can be treated as totals {[}1167{]}.

\item {} 
Fixed bug in ecl\_kw\_compare\_numeric( ) {[}1168{]}.

\item {} 
Refactored / inlined volume calculations in ecl\_grid {[}1173, 1184{]}.

\item {} 
Made function ecl\_kw\_size\_and\_type\_equal() public {[}1192{]}.

\item {} 
Fixed bug in ecl\_grid\_cell\_contains() {[}1402, 1404, 1195, 1419{]}.

\item {} 
OOM bug in ecl\_kw\_grdecl loader for large files {[}1207{]}.

\item {} 
Cache cell volumes in ecl\_grid {[}1228{]}.

\item {} 
Geertsma / gravity functionality {[}1227, 1284, 1289, 1292, 1364, 1408{]}.

\item {} 
Summary + restart - will allow some keyword differences {[}1296{]}.

\item {} 
Implemented ecl\_rst\_file\_open\_write\_seek( ) {[}1236{]}.

\item {} 
Optionally apply mapaxes {[}1242, 1281{]}.

\item {} 
Expose and use ecl\_file\_view datastructere - stop using ‘blocks’ in ecl\_file objects {[}1250{]}.

\item {} 
ecl\_sum will internalize Aquifer variables {[}1275{]}.

\item {} 
Make sure region properties RxxT are marked as total + depreecated some properties {[}1285{]}.

\item {} 
ecl\_kw\_resize() + C++ operator{[}{]} {[}1316{]}

\item {} 
Added small C++ utility to create eclipse filenames{[}1396{]}.

\item {} 
Make sure restart and INIT files are written with correct unit ID {[}1399, 1407{]}.

\item {} 
Skip keyword data type: ‘C010’ without failing {[}1406, 1410{]}.

\item {} 
Adding parsing of the last (optional) config token for the SLAVES kwd {[}1409{]}.

\item {} 
Add nnc index to the information exported by ecl\_nnc\_export() {[}1204{]}.

\item {} 
Marked solvent related total keywords ?NIT and ?NPT.* as totals {[}1241{]}.

\item {} 
Top active cell in grid {[}1322{]}.

\item {} 
Added absolute epsilon to ecl\_kw comparsion {[}1345,1351{]}.

\end{itemize}

Smoother, updates and ‘running’:
\begin{itemize}
\item {} 
Fixed bug with local updates of GEN\_DATA {[}1291{]}.

\item {} 
Changed default MDA weights and removed file input {[}1156, 1190, 1191{]}.

\item {} 
Bug in handling of failed realisations {[}1163{]}.

\item {} 
Fix bug missing assignment of analysis module in ES-MDA {[}1179{]}.

\item {} 
OpenMP implementation of fwd\_step {[}1185, 1324,1342{]}.

\item {} 
Removes the ability to update dynamic variables {[}1189{]}.

\item {} 
Allow max CV FOLD to be the number of ensembles {[}1205, 1208{]}.

\item {} 
Fix for min\_realizations logic {[}1206{]}.

\item {} 
Can assign a specific analyis module for one local update {[}1224{]}.

\item {} 
Handle updates when some summary relaisations are “too short” {[}1400, 1405{]}.

\item {} 
Extending hook manager to support PRE\_UPDATE and POST\_UPDATE hooks {[}1340,1360{]}.

\item {} 
RML logging is on by default {[}1318{]}.

\item {} 
Changed default ENKF\_ALPHA value to 3.0 {[}??{]}

\item {} 
Implemented subsspacce inversion algorithm {[}1334, 1344{]}.

\end{itemize}

libgeometry:
\begin{itemize}
\item {} 
Added function to create new geo\_surface (i.e. IRAP) object {[}1308{]}.

\item {} 
Get (x, y) pos from surface {[}1395{]}.

\end{itemize}

Python code:
\begin{itemize}
\item {} 
cwrap has been moved out to stand-alone module, out of ert
package {[}1159, 1320, 1325, 1326, 1328, 1332, 1338, 1341, 1343, 1347, 1350, 1353{]}

\item {} 
Simplified loading of shared libraries {[}1234{]}.

\item {} 
Python3 preparations {[}1231, 1347{]}.

\item {} 
Added \_\_repr\_\_ methods: {[}1266, 1327, 1329, 1331, 1348, 1373, 1375, 1377, 1384, 1387{]}.

\item {} 
Implement \_\_getitem\_\_( ) for gen\_data {[}1331{]}.

\item {} 
Removed cstring\_obj Python class {[}1387{]}.

\item {} 
EclKW.numpy\_array returns shared buffer ndarray {[}1180{]}.

\item {} 
Minor bug in ecl\_kw.py {[}1171{]}.

\item {} 
Added EclKW.numpyView( ) and EclKW.numpyCopy( ) {[}1188{]}.

\item {} 
Bug in EclKW slice access {[}1203{]}.

\item {} 
Expose active\_list better in Python {[}1392{]}.

\item {} 
@TYPE@\_vector suppports negative indices in \_\_getitem\_\_ and
\_\_setitem\_\_; added repr method {[}1378{]}.

\item {} 
added root() methdo ert/\_\_init\_\_.py {[}1293{]}.

\end{itemize}

GUI / Configuration / Documentation
\begin{itemize}
\item {} 
Bug when viewing plots while simulating {[}1157.{]}

\item {} 
Bug when plotting short vectors {[}1303{]}.

\item {} 
Completely refactored the ERT Gui event system {[}1158, 1162{]}.

\item {} 
Marked keywords INIT\_SECTION and SCHEDULE\_FILE as deprecated {[}1181{]}.

\item {} 
Removed outdated keywords from documentation {[}1390{]}.

\item {} 
Documented UMASK keyword {[}1186{]}.

\item {} 
ConfigParser: Can turn off validation + warnings {[}1233, 1249, 1287{]}.

\item {} 
Make ies advanced option only {[}1401{]}.

\item {} 
Removing MAX\_RUNNING\_LOCAL and MAX\_RUNNING\_LSF from user doc {[}1398{]}.

\item {} 
Apply plot style to other plots {[}1397{]}.

\item {} 
Fig bug in initialization when out of range {[}1394{]}.

\item {} 
Added new object for generic config settings {[}1391{]}.

\item {} 
Changes to plot settings {[}11359,376,1380,1382,1386{]}.

\item {} 
Fix bug in load case manually {[}1368{]}.

\item {} 
Documentation of plugins {[}1194{]}.

\item {} 
Changed all time handling to UTC. This will affect loading old cases {[}1229, 1257{]}.

\item {} 
Removed keyword QC\_PATH + updated GRID {[}1263{]}.

\item {} 
Making sure the ertshell is creating the run path {[}1280{]}.

\item {} 
Create Doxygen {[}1277,1278,1294,1309,1317{]}.

\item {} 
Ability to run analysis from GUI {[}1314{]}.

\item {} 
Improved documentation of priors {[}1319{]}.

\item {} 
Bug in config parsing with relative paths {[}1333{]}.

\item {} 
Field documentation updates {[}1337{]}.

\end{itemize}

libwecl\_well:
\begin{itemize}
\item {} 
Internalize rates for wells and connections in the well library
{[}1403{]}.

\item {} 
New function well\_ts\_get\_name() {[}1393{]}.

\end{itemize}

libutil:
\begin{itemize}
\item {} 
Functions for parsing and outputting dates in ISO format{[}1248{]}.

\item {} 
stringlist\_join - like Python str.join {[}1243{]}.

\item {} 
bug in matrix\_dgemm {[}1286{]}.

\item {} 
Resurrected block\_fs utilities from the past {[}1297{]}.

\item {} 
Slicing for runpath\_list {[}1356{]}.

\end{itemize}


\chapter{Indices and tables}
\label{\detokenize{manual:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}